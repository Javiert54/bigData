{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41969ed",
   "metadata": {},
   "source": [
    "### Implementación de un Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96494a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3b089",
   "metadata": {},
   "source": [
    "### 1. Codificación Posicional (Positional Encoding)\n",
    "\n",
    "La codificación posicional añade información sobre la posición relativa o absoluta de los tokens en la secuencia. Las funciones sinusoidales fueron elegidas porque pueden permitir al modelo aprender a atender por posición relativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06795778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa la codificación posicional sinusoidal.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) # (d_model/2)\n",
    "        \n",
    "        pe = torch.zeros(max_len, 1, d_model) # (max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # x se espera que sea (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c8028",
   "metadata": {},
   "source": [
    "### 2. Atención Multi-Cabeza (Multi-Head Attention)\n",
    "\n",
    "Permite al modelo atender conjuntamente a información de diferentes subespacios\n",
    "de representación en diferentes posiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e6f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa la atención multi-cabeza.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model debe ser divisible por num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale_factor = math.sqrt(self.d_k)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9) \n",
    "            \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        \n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # query, key, value: (batch_size, seq_len, d_model)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        context = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890de18",
   "metadata": {},
   "source": [
    "### 3. Redes Feed-Forward (Position-wise Feed-Forward Networks)\n",
    "\n",
    "Consiste en dos transformaciones lineales con una activación ReLU entre ellas, \n",
    "aplicadas independientemente a cada posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4586c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementa la red feed-forward (FFN) aplicada a cada posición.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4f5e8",
   "metadata": {},
   "source": [
    "### 4. Capa del Codificador (Encoder Layer)\n",
    "\n",
    "Una capa del codificador consiste en una subcapa de atención multi-cabeza (self-attention) \n",
    "y una subcapa de red feed-forward. Se utilizan conexiones residuales y normalización de capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ae9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Una capa del codificador.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output = self.self_attn(src, src, src, src_mask)\n",
    "        src = self.norm1(src + self.dropout1(attn_output))\n",
    "\n",
    "        ff_output = self.feed_forward(src)\n",
    "        src = self.norm2(src + self.dropout2(ff_output))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f69118",
   "metadata": {},
   "source": [
    "### 5. Capa del Decodificador (Decoder Layer)\n",
    "\n",
    "Similar a la capa del codificador, pero con una subcapa adicional que realiza \n",
    "atención multi-cabeza sobre la salida del codificador. La self-attention del \n",
    "decodificador está enmascarada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dfcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Una capa del decodificador.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, \n",
    "                tgt_mask: torch.Tensor = None, memory_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout1(attn_output))\n",
    "\n",
    "        enc_dec_attn_output = self.enc_dec_attn(tgt, memory, memory, memory_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout2(enc_dec_attn_output))\n",
    "\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout3(ff_output))\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68a2d1",
   "metadata": {},
   "source": [
    "### 6. Codificador (Encoder)\n",
    "\n",
    "El codificador es una pila de N capas EncoderLayer idénticas. Incluye la capa \n",
    "de embedding y la codificación posicional para la secuencia de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e834dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    El codificador es una pila de N capas EncoderLayer idénticas.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int,\n",
    "                 input_vocab_size: int, max_seq_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "                                     for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # src: (batch_size, src_seq_len)\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        # PositionalEncoding espera (seq_len, batch_size, d_model)\n",
    "        src_emb = src_emb.transpose(0, 1) # (src_seq_len, batch_size, d_model)\n",
    "        src_emb_pos = self.pos_encoder(src_emb)\n",
    "        src_emb_pos = src_emb_pos.transpose(0, 1) # (batch_size, src_seq_len, d_model)\n",
    "        \n",
    "        output = self.dropout(src_emb_pos)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a95c4",
   "metadata": {},
   "source": [
    "### 7. Decodificador (Decoder)\n",
    "\n",
    "El decodificador es también una pila de N capas DecoderLayer idénticas. \n",
    "Incluye la capa de embedding y la codificación posicional para la secuencia objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19798a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    El decodificador es también una pila de N capas DecoderLayer idénticas.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int,\n",
    "                 output_vocab_size: int, max_seq_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "                                     for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, \n",
    "                tgt_mask: torch.Tensor = None, memory_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # tgt: (batch_size, tgt_seq_len)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        # PositionalEncoding espera (seq_len, batch_size, d_model)\n",
    "        tgt_emb = tgt_emb.transpose(0,1) # (tgt_seq_len, batch_size, d_model)\n",
    "        tgt_emb_pos = self.pos_encoder(tgt_emb)\n",
    "        tgt_emb_pos = tgt_emb_pos.transpose(0,1) # (batch_size, tgt_seq_len, d_model)\n",
    "        \n",
    "        output = self.dropout(tgt_emb_pos)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, tgt_mask, memory_mask)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3777663",
   "metadata": {},
   "source": [
    "### 8. Modelo Transformer Completo\n",
    "\n",
    "Une el codificador y el decodificador, y añade una capa lineal final para la \n",
    "predicción de tokens. También incluye métodos para crear las máscaras necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50194aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo Transformer completo.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int, \n",
    "                 num_heads: int, num_encoder_layers: int, num_decoder_layers: int, \n",
    "                 d_ff: int, max_seq_len: int, dropout: float = 0.1, pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx # Guardamos el índice de padding\n",
    "\n",
    "        self.encoder = Encoder(num_encoder_layers, d_model, num_heads, d_ff, \n",
    "                               src_vocab_size, max_seq_len, dropout)\n",
    "        self.decoder = Decoder(num_decoder_layers, d_model, num_heads, d_ff, \n",
    "                               tgt_vocab_size, max_seq_len, dropout)\n",
    "        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def make_src_mask(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        # src: (batch_size, src_seq_len)\n",
    "        # (batch_size, 1, 1, src_seq_len)\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        # tgt: (batch_size, tgt_seq_len)\n",
    "        # (batch_size, 1, 1, tgt_seq_len)\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        tgt_len = tgt.size(1)\n",
    "        # (1, 1, tgt_seq_len, tgt_seq_len)\n",
    "        look_ahead_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device)).bool()\n",
    "        look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        tgt_mask = tgt_pad_mask & look_ahead_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        memory_mask = src_mask # Para la atención encoder-decoder, la máscara de K,V es la de src\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, tgt_mask, memory_mask)\n",
    "        \n",
    "        output = self.final_linear(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedda07a",
   "metadata": {},
   "source": [
    "### Explicación de Decisiones de Diseño"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ff5a2",
   "metadata": {},
   "source": [
    "#### 1. Elección de Hiperparámetros (Ejemplos y Justificación):\n",
    "*   **`d_model` (Dimensión del Modelo)**: e.g., 256 o 512. Es la dimensionalidad de los embeddings y las capas internas. Un valor mayor captura más información pero aumenta el costo computacional. Para un modelo 'sencillo', 256 podría ser un buen inicio.\n",
    "*   **`num_heads` (Número de Cabezas de Atención)**: e.g., 8. `d_model` debe ser divisible por `num_heads`. Permite al modelo atender a diferentes partes de la información simultáneamente. `d_k = d_model / num_heads` es la dimensión de cada cabeza.\n",
    "*   **`num_encoder_layers` / `num_decoder_layers` (Número de Capas)**: e.g., 2 a 3 (para un modelo sencillo). Más capas permiten aprender representaciones más complejas.\n",
    "*   **`d_ff` (Dimensión de la Red Feed-Forward Interna)**: e.g., 1024 o 2048 (típicamente 4 * `d_model`). La expansión en esta capa permite al modelo aprender transformaciones más ricas.\n",
    "*   **`dropout_rate` (Tasa de Dropout)**: e.g., 0.1. Ayuda a prevenir el sobreajuste.\n",
    "*   **`max_seq_len` (Longitud Máxima de Secuencia)**: e.g., 100 o 512. Necesario para la codificación posicional precalculada.\n",
    "*   **`vocab_size` (Tamaño del Vocabulario)**: Depende del corpus. Determina el tamaño de la capa de embedding y la capa lineal final.\n",
    "*   **`pad_idx` (Índice de Padding)**: El índice usado para rellenar secuencias. Importante para las máscaras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41992809",
   "metadata": {},
   "source": [
    "#### 2. Estructura de la Red:\n",
    "*   **Embedding Layer**: Convierte tokens en vectores densos (`d_model`). Se escala por `sqrt(d_model)`.\n",
    "*   **Positional Encoding**: Sumado a embeddings para información de posición (sinusoidal).\n",
    "*   **Multi-Head Attention**:\n",
    "    *   *Scaled Dot-Product Attention*: Calcula pesos de atención, escalados por `sqrt(d_k)`.\n",
    "    *   *Múltiples Cabezas*: Proyecciones Q, K, V divididas, atención en paralelo, resultados concatenados.\n",
    "*   **Add & Norm (Conexiones Residuales y Normalización de Capa)**:\n",
    "    *   *Conexiones Residuales*: `x + Sublayer(x)`. Mitigan gradiente desvaneciente.\n",
    "    *   *Layer Normalization*: Normaliza activaciones, estabiliza y acelera entrenamiento.\n",
    "*   **Position-wise Feed-Forward Network (FFN)**: Dos capas lineales con ReLU, aplicadas por posición.\n",
    "*   **Encoder**: Pila de `num_encoder_layers` EncoderLayer. Procesa entrada.\n",
    "*   **Decoder**: Pila de `num_decoder_layers` DecoderLayer. Genera salida.\n",
    "    *   *Masked Multi-Head Self-Attention*: Previene atender a tokens futuros en el decodificador.\n",
    "    *   *Encoder-Decoder Attention*: Decodificador atiende a salida del codificador (Q de decoder, K,V de encoder).\n",
    "*   **Capa Lineal Final + Softmax**: Proyecta salida del decodificador a `tgt_vocab_size`. Softmax (implícito en `CrossEntropyLoss`) para probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b9963",
   "metadata": {},
   "source": [
    "#### 3. Funciones de Activación Utilizadas:\n",
    "*   **ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`.\n",
    "    *   *Ubicación*: En FFN.\n",
    "    *   *Justificación*: Eficiente, mitiga gradiente desvaneciente, no linealidad.\n",
    "*   **Softmax**: `softmax(z_i) = exp(z_i) / sum(exp(z_j))`.\n",
    "    *   *Ubicación*:\n",
    "        1.  Dentro de Scaled Dot-Product Attention (para pesos de atención).\n",
    "        2.  Capa de salida final (para probabilidades sobre vocabulario, a menudo implícita en la función de pérdida).\n",
    "    *   *Justificación*: Normaliza a distribución de probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce28fc",
   "metadata": {},
   "source": [
    "### Ejemplo de Uso Sencillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0deed2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "\n",
      "Forma de src_tokens: torch.Size([2, 10])\n",
      "Forma de tgt_tokens: torch.Size([2, 12])\n",
      "Forma de la salida (logits): torch.Size([2, 12, 1200])\n",
      "\n",
      "¡Implementación y ejemplo completados!\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros para el ejemplo\n",
    "SRC_VOCAB_SIZE = 1000\n",
    "TGT_VOCAB_SIZE = 1200\n",
    "D_MODEL = 256 \n",
    "NUM_HEADS = 8\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "D_FF = 512 \n",
    "MAX_SEQ_LEN = 80\n",
    "DROPOUT = 0.1\n",
    "PAD_IDX = 0 # Índice para el token de padding\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Crear instancia del modelo Transformer\n",
    "model = Transformer(src_vocab_size=SRC_VOCAB_SIZE,\n",
    "                    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "                    d_model=D_MODEL,\n",
    "                    num_heads=NUM_HEADS,\n",
    "                    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                    d_ff=D_FF,\n",
    "                    max_seq_len=MAX_SEQ_LEN,\n",
    "                    dropout=DROPOUT,\n",
    "                    pad_idx=PAD_IDX)\n",
    "\n",
    "# Mover modelo a GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "model = model.to(device)\n",
    "model.eval() # Poner en modo evaluación\n",
    "\n",
    "# Crear datos de entrada ficticios\n",
    "BATCH_SIZE = 2\n",
    "SRC_SEQ_LEN = 10\n",
    "TGT_SEQ_LEN = 12 \n",
    "\n",
    "src_tokens = torch.randint(1, SRC_VOCAB_SIZE, (BATCH_SIZE, SRC_SEQ_LEN), device=device)\n",
    "tgt_tokens = torch.randint(1, TGT_VOCAB_SIZE, (BATCH_SIZE, TGT_SEQ_LEN), device=device)\n",
    "\n",
    "# Simular padding\n",
    "if SRC_SEQ_LEN > 1:\n",
    "    src_tokens[0, -1] = PAD_IDX\n",
    "if TGT_SEQ_LEN > 1:\n",
    "    tgt_tokens[0, -1] = PAD_IDX\n",
    "\n",
    "print(f\"\\nForma de src_tokens: {src_tokens.shape}\")\n",
    "print(f\"Forma de tgt_tokens: {tgt_tokens.shape}\")\n",
    "\n",
    "# Pasar los datos a través del modelo\n",
    "with torch.no_grad():\n",
    "    output_logits = model(src_tokens, tgt_tokens)\n",
    "\n",
    "print(f\"Forma de la salida (logits): {output_logits.shape}\")\n",
    "\n",
    "print(\"\\n¡Implementación y ejemplo completados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b13bf6",
   "metadata": {},
   "source": [
    "Este es un Transformer 'sencillo' que cubre los componentes fundamentales. Para un uso práctico, se necesitaría un tokenizer, un conjunto de datos, un bucle de entrenamiento, una función de pérdida (e.g., `CrossEntropyLoss`), y un optimizador (e.g., Adam)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
