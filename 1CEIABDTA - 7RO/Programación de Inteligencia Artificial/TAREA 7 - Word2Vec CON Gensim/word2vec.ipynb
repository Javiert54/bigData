{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from gensim.models import Word2Vec, KeyedVectors  # Gensim library for Word2Vec model\n",
    "import pandas as pd  # Pandas library for data manipulation\n",
    "import nltk  # NLTK library for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>date_created</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1201232046</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1201232075</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201232523</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "      <td>False</td>\n",
       "      <td>polar</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1201233290</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "      <td>False</td>\n",
       "      <td>fadi420</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1201274720</td>\n",
       "      <td>2008-01-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "      <td>False</td>\n",
       "      <td>mhermans</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_created date_created  up_votes  down_votes  \\\n",
       "0    1201232046   2008-01-25         3           0   \n",
       "1    1201232075   2008-01-25         2           0   \n",
       "2    1201232523   2008-01-25         3           0   \n",
       "3    1201233290   2008-01-25         1           0   \n",
       "4    1201274720   2008-01-25         4           0   \n",
       "\n",
       "                                             title  over_18    author  \\\n",
       "0                Scores killed in Pakistan clashes    False     polar   \n",
       "1                 Japan resumes refuelling mission    False     polar   \n",
       "2                  US presses Egypt on Gaza border    False     polar   \n",
       "3     Jump-start economy: Give health care to all     False   fadi420   \n",
       "4  Council of Europe bashes EU&UN terror blacklist    False  mhermans   \n",
       "\n",
       "   subreddit  \n",
       "0  worldnews  \n",
       "1  worldnews  \n",
       "2  worldnews  \n",
       "3  worldnews  \n",
       "4  worldnews  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('reddit_worldnews_start_to_2016-11-22.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 'punkt' tokenizer models from NLTK\n",
    "# Uncomment the lines below to download the models if not already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Scores', 'killed', 'in', 'Pakistan', 'clashes'],\n",
       " ['Japan', 'resumes', 'refuelling', 'mission'],\n",
       " ['US', 'presses', 'Egypt', 'on', 'Gaza', 'border'],\n",
       " ['Jump-start', 'economy', ':', 'Give', 'health', 'care', 'to', 'all'],\n",
       " ['Council', 'of', 'Europe', 'bashes', 'EU', '&', 'UN', 'terror', 'blacklist']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the titles in the dataframe using NLTK's word_tokenize function\n",
    "newsVec = [nltk.word_tokenize(title) for title in df['title'].values]\n",
    "\n",
    "# Display the first 5 tokenized titles to verify the result\n",
    "newsVec[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model using the tokenized titles (newsVec)\n",
    "# min_count=1 ensures that even words that appear only once are included in the model\n",
    "model = Word2Vec(newsVec, min_count=1).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9072763919830322),\n",
       " ('teenager', 0.8432095646858215),\n",
       " ('boy', 0.8381759524345398),\n",
       " ('girl', 0.824225127696991),\n",
       " ('couple', 0.7924955487251282),\n",
       " ('teen', 0.7605805397033691),\n",
       " ('mother', 0.7585114240646362),\n",
       " ('policeman', 0.7574959397315979),\n",
       " ('doctor', 0.7536249160766602),\n",
       " ('teacher', 0.7390760779380798)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to 'man' using the trained Word2Vec model\n",
    "similar_words = model.most_similar('man')\n",
    "\n",
    "# Display the similar words\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holiday', 0.8276294469833374),\n",
       " ('holidays', 0.7679221630096436),\n",
       " ('gifts', 0.7360822558403015),\n",
       " ('Christmas', 0.7285211682319641),\n",
       " ('winter', 0.6602602005004883),\n",
       " ('festive', 0.6454193592071533),\n",
       " ('Thanksgiving', 0.6245847940444946),\n",
       " ('gift', 0.612133264541626),\n",
       " ('Christmastime', 0.6087110042572021),\n",
       " ('vacations', 0.5980472564697266)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector by adding the vectors for 'holiday', 'gifts', and 'winter'\n",
    "vec = model['holiday'] + model['gifts'] + model['winter']\n",
    "\n",
    "# Find the most similar words to the created vector\n",
    "similar_words = model.most_similar([vec])\n",
    "\n",
    "# Display the similar words\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el archivo y lo descomprimimos en la raiz del proyecto:\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model from the Google News dataset\n",
    "# The model is in binary format, so we set binary=True\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.52085942029953),\n",
       " ('woman', 0.5135486721992493),\n",
       " ('monarch', 0.48635631799697876),\n",
       " ('crown_prince', 0.47217562794685364),\n",
       " ('prince', 0.4661101698875427),\n",
       " ('princess', 0.45525479316711426),\n",
       " ('man', 0.4482707381248474),\n",
       " ('teenage_girl', 0.4421442151069641),\n",
       " ('girl', 0.42170172929763794),\n",
       " ('boy', 0.40749162435531616)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector by adding the vectors for 'king' and 'woman' and subtracting the vector for 'man'\n",
    "vec = model['king'] + model['woman'] - model['man']\n",
    "\n",
    "# Find the most similar words to the created vector\n",
    "similar_words = model.most_similar([vec])\n",
    "\n",
    "# Display the similar words\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('winter', 0.3277788758277893),\n",
       " ('Thanksgiving', 0.2999882400035858),\n",
       " ('holiday', 0.2999754250049591),\n",
       " ('holidays', 0.2830084264278412),\n",
       " ('Labor_Day', 0.2706221342086792),\n",
       " ('summertime', 0.2701828181743622),\n",
       " ('springtime', 0.2672084867954254),\n",
       " ('wintry_weather', 0.26370859146118164),\n",
       " ('summer', 0.25874072313308716),\n",
       " ('spring', 0.2562393844127655)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector by adding the vectors for 'holiday' and 'winter' and subtracting the vector for 'summer'\n",
    "vec = model['holiday'] + model['winter'] - model['summer']\n",
    "\n",
    "# Find the most similar words to the created vector\n",
    "similar_words = model.most_similar([vec])\n",
    "\n",
    "# Display the similar words\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
