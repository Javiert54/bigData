{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fab16d",
   "metadata": {},
   "source": [
    "# Q-Learning para MountainCarContinuous discreto\n",
    "\n",
    "Este notebook implementa el algoritmo Q-Learning para resolver el entorno `MountainCarContinuous-v0` de OpenAI Gym.\n",
    "\n",
    "**Objetivo:** Enseñar a un agente a conducir un coche por un valle para alcanzar la bandera en la cima de la colina derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc94d2",
   "metadata": {},
   "source": [
    "## 1. Importaciones\n",
    "\n",
    "Importamos las librerías necesarias: `gym` para el entorno, `numpy` para cálculos numéricos, `matplotlib` para gráficas, `pathlib` para manejar rutas de archivos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf08c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') # O Qt5Agg si no va TkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers import RecordVideo\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd42e",
   "metadata": {},
   "source": [
    "## 2. Configuración y Parámetros\n",
    "\n",
    "Definimos las constantes y hiperparámetros para el entorno, el entrenamiento, la discretización, la grabación de vídeos y la generación de gráficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1574f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración General ---\n",
    "ENV_NAME = 'MountainCarContinuous-v0'\n",
    "VIDEO_FOLDER = Path('videos') # Carpeta para vídeos (nombre diferente para no mezclar)\n",
    "VIDEO_FOLDER.mkdir(parents=True, exist_ok=True) # Crear carpeta si no existe\n",
    "\n",
    "# --- Parámetros del Training (Q-Learning) ---\n",
    "EPISODES = 6000\n",
    "MAX_STEPS = 900 # Máximos pasos por episodio\n",
    "LR = 0.15 # Learning Rate (Alpha)\n",
    "GAMMA = 0.99 # Discount Factor\n",
    "EPSILON = 1.0 # Epsilon inicial (exploración)\n",
    "EPSILON_DECAY = 0.9994 # Factor de decaimiento de epsilon\n",
    "MIN_EPSILON = 0.05 # Epsilon mínimo\n",
    "\n",
    "# --- Parámetros de Reward Shaping ---\n",
    "STEP_PENALTY = 0.0 # Castigo por cada paso (además de la recompensa base del entorno)\n",
    "STRONG_ACTION_REWARD = 0.02 # Premio por usar acciones de mayor magnitud\n",
    "BOTTOM_POS = -0.5  # Posición del fondo del valle\n",
    "MAX_PENALTY_BOTTOM = 1.0 # Castigo máximo por estar quieto en el fondo\n",
    "POS_SENSITIVITY = 60.0 # Sensibilidad del castigo a la posición\n",
    "VEL_SENSITIVITY = 1100.0 # Sensibilidad del castigo a la velocidad (cerca de 0)\n",
    "\n",
    "# --- Discretización del Espacio de Estados ---\n",
    "NUM_BINS = (10, 10) # Número de 'cajas' para (posición, velocidad)\n",
    "\n",
    "# --- Discretización del Espacio de Acciones ---\n",
    "# El entorno continuo acepta una fuerza entre -1.0 y 1.0.\n",
    "# Discretizamos este espacio para poder usar Q-Learning tabular.\n",
    "ACTIONS = [-0.7, -0.5, -0.3, 0.0, 0.3, 0.5, 0.7] # Acciones discretas\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "# --- Configuración de Grabación de Vídeos ---\n",
    "RECORD_START = 0 # Episodio desde el que empezar a considerar grabar\n",
    "RECORD_END = EPISODES # Episodio hasta el que considerar grabar\n",
    "RECORD_EVERY = 500 # Grabar el mejor episodio de cada X episodios\n",
    "\n",
    "# --- Configuración de la Gráfica ---\n",
    "PLOT_START = RECORD_START # Episodio inicial para la gráfica\n",
    "PLOT_END = RECORD_END # Episodio final para la gráfica\n",
    "AVG_WINDOW = 50 # Ventana para la media móvil en la gráfica\n",
    "PLOT_NAME = \"grafica_recompensas_montaña_notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e7e6f",
   "metadata": {},
   "source": [
    "## 3. Funciones Auxiliares\n",
    "\n",
    "Definimos funciones para:\n",
    "*   `preparar_entorno`: Crear la instancia del entorno y calcular los límites para la discretización.\n",
    "*   `crear_q_table`: Inicializar la tabla Q con ceros.\n",
    "*   `discretizar_estado`: Convertir un estado continuo (posición, velocidad) en índices discretos para la tabla Q.\n",
    "*   `elegir_accion`: Implementar la política epsilon-greedy para seleccionar la siguiente acción.\n",
    "*   `jugar_y_grabar`: Ejecutar un episodio usando la política aprendida (sin exploración) y grabarlo en vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c72827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_entorno(env_name, num_bins):\n",
    "    \"\"\"Crea el entorno y calcula las divisiones para el estado.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    low = env.observation_space.low\n",
    "    high = env.observation_space.high\n",
    "    high_adj = high + 1e-6 # Pequeño ajuste para incluir el límite superior en linspace\n",
    "    \n",
    "    state_bins = []\n",
    "    for i in range(env.observation_space.shape[0]):\n",
    "        bins = np.linspace(low[i], high_adj[i], num=num_bins[i] + 1)[1:-1]\n",
    "        state_bins.append(bins)\n",
    "        \n",
    "    print(\"Límites del estado:\", low, high)\n",
    "    print(\"Divisiones para discretizar (bordes internos):\")\n",
    "    for i, b in enumerate(state_bins):\n",
    "        print(f\"  Dimensión {i}: {len(b)+1} cajas ({num_bins[i]} divisiones internas)\")\n",
    "    return env, state_bins\n",
    "\n",
    "def crear_q_table(num_bins, num_actions):\n",
    "    \"\"\"Inicializa la tabla Q con ceros.\"\"\"\n",
    "    q_table_size = num_bins + (num_actions,)\n",
    "    q_table = np.zeros(q_table_size)\n",
    "    print(f\"Tabla Q creada con tamaño: {q_table.shape}\")\n",
    "    return q_table\n",
    "\n",
    "def discretizar_estado(estado, state_bins, num_bins_config):\n",
    "    \"\"\"Convierte un estado continuo a una tupla de índices (caja_pos, caja_vel).\"\"\"\n",
    "    indices = []\n",
    "    # Asegurarse de que el estado es un array numpy\n",
    "    estado_np = np.asarray(estado)\n",
    "    for i in range(len(estado_np)):\n",
    "        # digitize nos dice en qué caja cae el valor\n",
    "        idx = int(np.digitize(estado_np[i], state_bins[i]))\n",
    "        # Asegurarse de que el índice está dentro de los límites [0, num_bins[i]-1]\n",
    "        idx_clipped = np.clip(idx, 0, num_bins_config[i] - 1)\n",
    "        indices.append(idx_clipped)\n",
    "    return tuple(indices)\n",
    "\n",
    "def elegir_accion(estado_idx, q_table, epsilon, num_actions):\n",
    "    \"\"\"Elige acción: epsilon-greedy.\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, num_actions) # Explorar\n",
    "    else:\n",
    "        return np.argmax(q_table[estado_idx]) # Explotar\n",
    "\n",
    "def jugar_y_grabar(env, q_table, state_bins, actions, num_actions, max_steps, video_folder, ep, reward, num_bins_config):\n",
    "    \"\"\"Juega un episodio usando solo lo aprendido (sin explorar) y lo graba.\"\"\"\n",
    "\n",
    "    reward_str = f\"reward_{reward:.2f}\".replace('.', '_')\n",
    "    filename = f\"mountaincar-mejor-ep{ep}-{reward_str}\"\n",
    "    print(f\"--- Grabando episodio {ep} (Recompensa: {reward:.2f})... ---\")\n",
    "\n",
    "    record_env = None # Inicializar por si falla la creación del wrapper\n",
    "    try:\n",
    "        # Usamos lambda e_idx: e_idx == 0 para grabar solo el primer episodio que se le pasa\n",
    "        record_env = RecordVideo(env, str(video_folder), episode_trigger=lambda e_idx: e_idx == 0, name_prefix=filename)\n",
    "\n",
    "        # Resetear el entorno envuelto\n",
    "        obs, info = record_env.reset()\n",
    "\n",
    "        # --- CORRECCIÓN: Eliminada la llamada a record_env.render() de aquí ---\n",
    "        # No es necesaria y causaba el error 'last_frame'.\n",
    "        # El wrapper RecordVideo captura los frames durante las llamadas a step().\n",
    "\n",
    "        # Asegúrate de pasar num_bins_config si tu función lo requiere\n",
    "        estado = discretizar_estado(obs, state_bins, num_bins_config)\n",
    "        terminado = False\n",
    "        truncado = False # Para gym >= 0.26\n",
    "        pasos = 0\n",
    "\n",
    "        # Bucle del episodio (solo explotación)\n",
    "        for t in range(max_steps):\n",
    "            accion_idx = np.argmax(q_table[estado]) # Elegir la mejor acción\n",
    "            accion_continua = np.array([actions[accion_idx]], dtype=np.float32)\n",
    "\n",
    "            try:\n",
    "                # La llamada a step() es la que internamente hace que se capture el frame\n",
    "                obs_siguiente, rec, terminado, truncado, info = record_env.step(accion_continua)\n",
    "            except Exception as e:\n",
    "                print(f\"Error en step grabando ep {ep}, paso {t}: {e}\")\n",
    "                terminado = True # Forzar fin si hay error en step\n",
    "                break # Salir del bucle de pasos\n",
    "\n",
    "            # Asegúrate de pasar num_bins_config si tu función lo requiere\n",
    "            estado_siguiente = discretizar_estado(obs_siguiente, state_bins, num_bins_config)\n",
    "            estado = estado_siguiente\n",
    "            pasos = t + 1\n",
    "\n",
    "            if terminado or truncado:\n",
    "                break\n",
    "\n",
    "        print(f\"--- Grabación ep {ep} terminada ({pasos} pasos). ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Captura errores durante la inicialización o el bucle de grabación\n",
    "        print(f\"Error al preparar o grabar el vídeo del episodio {ep}: {e}\")\n",
    "        # traceback.print_exc() # Puedes descomentar para obtener un traceback más detallado\n",
    "\n",
    "    finally:\n",
    "        # Importante cerrar el entorno de grabación para que guarde el vídeo correctamente\n",
    "        if record_env is not None: # Comprobar si se llegó a crear el wrapper\n",
    "             try:\n",
    "                 record_env.close()\n",
    "                 print(f\"Entorno de grabación para ep {ep} cerrado.\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Error cerrando RecordVideo para ep {ep}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45dd224",
   "metadata": {},
   "source": [
    "## 4. Función Principal de Entrenamiento\n",
    "\n",
    "Esta función contiene el bucle principal de Q-Learning. Itera sobre los episodios, y en cada episodio, itera sobre los pasos. En cada paso:\n",
    "1. Elige una acción (epsilon-greedy).\n",
    "2. Ejecuta la acción en el entorno.\n",
    "3. Observa el nuevo estado y la recompensa.\n",
    "4. Aplica *reward shaping* (modifica la recompensa para guiar mejor el aprendizaje).\n",
    "5. Actualiza el valor Q correspondiente en la tabla usando la fórmula de Q-Learning.\n",
    "6. Actualiza el estado.\n",
    "7. Gestiona la grabación del mejor episodio cada cierto número de episodios.\n",
    "8. Reduce epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897d4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(env, q_table, state_bins, actions, num_actions, episodes, max_steps,\n",
    "             lr, gamma, start_epsilon, eps_decay, min_epsilon,\n",
    "             step_penalty, strong_action_reward,\n",
    "             bottom_pos, max_penalty_bottom, pos_sensitivity, vel_sensitivity,\n",
    "             video_folder, record_start, record_end, record_every, num_bins_config): # Añadido num_bins_config\n",
    "    \"\"\"Bucle principal de Q-learning con grabación periódica del mejor episodio.\"\"\"\n",
    "\n",
    "    todas_recompensas = []\n",
    "    epsilon = start_epsilon\n",
    "    mejor_recompensa_chunk = -np.inf\n",
    "    mejor_episodio_chunk = -1\n",
    "    episodios_en_chunk = 0\n",
    "\n",
    "    # Crear carpeta de vídeos si no existe (mejor hacerlo fuera, antes de llamar a entrenar, o al inicio del script)\n",
    "    # video_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n--- Empezando Entrenamiento ({episodes} episodios) ---\")\n",
    "    # ... (más prints informativos si quieres) ...\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        ep_num = ep + 1\n",
    "\n",
    "        try:\n",
    "            obs, info = env.reset()\n",
    "        except Exception as reset_e:\n",
    "             print(f\"Error reseteando entorno en episodio {ep_num}: {reset_e}\")\n",
    "             traceback.print_exc()\n",
    "             continue # Saltar episodio\n",
    "\n",
    "        # Asegúrate de pasar num_bins_config a discretizar_estado\n",
    "        estado = discretizar_estado(obs, state_bins, num_bins_config)\n",
    "        recompensa_total_episodio = 0.0\n",
    "        terminado = False\n",
    "        truncado = False\n",
    "        pasos = 0\n",
    "\n",
    "        # Bucle dentro de un episodio (pasos)\n",
    "        for t in range(max_steps):\n",
    "            accion_idx = elegir_accion(estado, q_table, epsilon, num_actions)\n",
    "            accion_continua = np.array([actions[accion_idx]], dtype=np.float32)\n",
    "\n",
    "            try:\n",
    "                obs_siguiente, recompensa_base, terminado, truncado, info = env.step(accion_continua)\n",
    "                recompensa = float(recompensa_base)\n",
    "                estado_siguiente_continuo = obs_siguiente\n",
    "                # Asegúrate de pasar num_bins_config a discretizar_estado\n",
    "                estado_siguiente = discretizar_estado(obs_siguiente, state_bins, num_bins_config)\n",
    "\n",
    "                # --- Reward Shaping ---\n",
    "                accion_valor = actions[accion_idx]\n",
    "                posicion = estado_siguiente_continuo[0]\n",
    "                velocidad = estado_siguiente_continuo[1]\n",
    "                recompensa += strong_action_reward * abs(accion_valor)\n",
    "                if not (terminado or truncado):\n",
    "                    recompensa -= step_penalty\n",
    "                factor_pos = math.exp(-pos_sensitivity * (posicion - bottom_pos)**2)\n",
    "                factor_vel = math.exp(-vel_sensitivity * velocidad**2)\n",
    "                castigo_quieto_abajo = max_penalty_bottom * factor_pos * factor_vel\n",
    "                recompensa -= castigo_quieto_abajo\n",
    "\n",
    "            except Exception as step_e:\n",
    "                print(f\"Error en step episodio {ep_num}, paso {t}: {step_e}\")\n",
    "                traceback.print_exc()\n",
    "                terminado = True\n",
    "                recompensa = 0.0\n",
    "                estado_siguiente = estado # Mantener estado si falló\n",
    "\n",
    "            # --- Actualización Q-Learning ---\n",
    "            recompensa_total_episodio += recompensa\n",
    "\n",
    "            valor_antiguo = q_table[estado + (accion_idx,)]\n",
    "            valor_max_siguiente = np.max(q_table[estado_siguiente]) if not (terminado or truncado) else 0.0\n",
    "            objetivo = recompensa + gamma * valor_max_siguiente\n",
    "            valor_nuevo = valor_antiguo + lr * (objetivo - valor_antiguo)\n",
    "            q_table[estado + (accion_idx,)] = valor_nuevo\n",
    "\n",
    "            estado = estado_siguiente\n",
    "            pasos = t + 1\n",
    "\n",
    "            if terminado or truncado:\n",
    "                break\n",
    "\n",
    "        todas_recompensas.append(recompensa_total_episodio)\n",
    "\n",
    "        # --- Lógica de Grabación (CORREGIDA INDENTACIÓN) ---\n",
    "        esta_en_rango_grabacion = record_start <= ep_num <= record_end\n",
    "        if esta_en_rango_grabacion:\n",
    "            episodios_en_chunk += 1\n",
    "            if recompensa_total_episodio >= mejor_recompensa_chunk:\n",
    "                mejor_recompensa_chunk = recompensa_total_episodio\n",
    "                mejor_episodio_chunk = ep_num\n",
    "\n",
    "            # Comprobar si grabar al final del chunk o del entrenamiento\n",
    "            fin_chunk = (episodios_en_chunk >= record_every)\n",
    "            ultimo_ep_rango = (ep_num == record_end)\n",
    "            ultimo_ep_total = (ep_num == episodes)\n",
    "\n",
    "            # --- ESTE BLOQUE AHORA ESTÁ DENTRO DEL if esta_en_rango_grabacion: ---\n",
    "            if (fin_chunk or ultimo_ep_rango or ultimo_ep_total) and mejor_episodio_chunk != -1:\n",
    "                 if record_start <= mejor_episodio_chunk <= record_end:\n",
    "                    # Crear un entorno nuevo para grabar es más seguro\n",
    "                    try:\n",
    "                        # Asegúrate que ENV_NAME está definido globalmente o pásalo como argumento\n",
    "                        env_grabacion = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "                        # Asegúrate que jugar_y_grabar acepta num_bins_config\n",
    "                        jugar_y_grabar(\n",
    "                            env=env_grabacion,\n",
    "                            q_table=q_table,\n",
    "                            state_bins=state_bins,\n",
    "                            actions=actions,\n",
    "                            num_actions=num_actions,\n",
    "                            max_steps=max_steps,\n",
    "                            video_folder=video_folder,\n",
    "                            ep=mejor_episodio_chunk,\n",
    "                            reward=mejor_recompensa_chunk,\n",
    "                            num_bins_config=num_bins_config # Pasar num_bins_config\n",
    "                        )\n",
    "                        env_grabacion.close()\n",
    "                    except Exception as record_setup_e:\n",
    "                         print(f\"Error al configurar/ejecutar grabación para ep {mejor_episodio_chunk}: {record_setup_e}\")\n",
    "                 # Resetear para el siguiente chunk (también indentado aquí)\n",
    "                 mejor_recompensa_chunk = -np.inf\n",
    "                 mejor_episodio_chunk = -1\n",
    "                 episodios_en_chunk = 0\n",
    "\n",
    "        # Decaimiento de Epsilon (fuera del if de grabación)\n",
    "        epsilon = max(min_epsilon, epsilon * eps_decay)\n",
    "\n",
    "        # Imprimir Progreso (fuera del if de grabación)\n",
    "        if ep_num % 100 == 0 or ep_num == episodes:\n",
    "            avg_reward_100 = np.mean(todas_recompensas[-100:]) if len(todas_recompensas) >= 100 else np.mean(todas_recompensas) if todas_recompensas else 0.0\n",
    "            print(f\"Episodio {ep_num}/{episodes} | \"\n",
    "                  f\"Recompensa: {recompensa_total_episodio:.2f} | \"\n",
    "                  f\"Media (últ 100): {avg_reward_100:.2f} | \"\n",
    "                  f\"Pasos: {pasos} | \"\n",
    "                  f\"Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    return todas_recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a9572",
   "metadata": {},
   "source": [
    "## 5. Función para Dibujar la Gráfica\n",
    "\n",
    "Esta función toma la lista de recompensas por episodio y genera una gráfica mostrando la recompensa de cada episodio y una media móvil para visualizar la tendencia del aprendizaje. Guarda la gráfica como un archivo PNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4851735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_grafica(recompensas, start_ep, end_ep, avg_window, filename):\n",
    "    \"\"\"Genera y guarda una gráfica de las recompensas por episodio y media móvil.\"\"\"\n",
    "    num_episodios = len(recompensas)\n",
    "    if num_episodios == 0:\n",
    "        print(\"No hay recompensas para dibujar.\")\n",
    "        return\n",
    "\n",
    "    start_idx = max(0, start_ep - 1)\n",
    "    end_idx = min(num_episodios, end_ep)\n",
    "\n",
    "    if start_idx >= end_idx:\n",
    "        print(f\"\\nRango de episodios para gráfica ({start_ep}-{end_ep}) no válido o sin datos.\")\n",
    "        return\n",
    "\n",
    "    recompensas_sub = recompensas[start_idx:end_idx]\n",
    "    episodios_sub = list(range(start_idx + 1, end_idx + 1))\n",
    "\n",
    "    if not episodios_sub:\n",
    "         print(f\"\\nNo hay episodios en el rango ({start_ep}-{end_ep}) para la gráfica.\")\n",
    "         return\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episodios_sub, recompensas_sub, label='Recompensa Episodio', alpha=0.6, linewidth=1)\n",
    "\n",
    "    # Calcular media móvil solo sobre el subconjunto visible\n",
    "    if len(recompensas_sub) >= avg_window:\n",
    "        media_movil_sub = np.convolve(recompensas_sub, np.ones(avg_window)/avg_window, mode='valid')\n",
    "        # Ajustar los episodios para la media móvil (empiezan después de la ventana inicial)\n",
    "        episodios_media_sub = list(range(start_idx + avg_window, end_idx + 1))\n",
    "        plt.plot(episodios_media_sub, media_movil_sub, \n",
    "                 label=f'Media Móvil ({avg_window} ep)', color='red', linewidth=2)\n",
    "    elif len(recompensas_sub) > 0:\n",
    "         # Si no hay suficientes datos para la ventana completa, calcular media simple\n",
    "         media_simple = np.mean(recompensas_sub)\n",
    "         plt.axhline(media_simple, color='orange', linestyle='--', label=f'Media Total Rango ({media_simple:.2f})')\n",
    "         print(f\"No hay suficientes datos ({len(recompensas_sub)}) para media móvil de {avg_window} episodios en el rango.\")\n",
    "\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Total Acumulada')\n",
    "    plt.title(f'Recompensas Q-Learning ({ENV_NAME} - Episodios {start_idx + 1} a {end_idx})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = f\"{filename}_ep_{start_idx + 1}_to_{end_idx}.png\"\n",
    "    try:\n",
    "        plt.savefig(output_file)\n",
    "        print(f\"\\nGráfica guardada como: '{output_file}'\")\n",
    "        # plt.show() # Mostrar la gráfica en el notebook\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError guardando/mostrando la gráfica '{output_file}': {e}\")\n",
    "    finally:\n",
    "        plt.close() # Cerrar la figura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e7379",
   "metadata": {},
   "source": [
    "## 6. Ejecución Principal\n",
    "\n",
    "Aquí es donde orquestamos todo el proceso:\n",
    "1. Preparamos el entorno y obtenemos los `state_bins`.\n",
    "2. Creamos la tabla Q.\n",
    "3. Llamamos a la función `entrenar` para iniciar el aprendizaje.\n",
    "4. Si el entrenamiento produce resultados, llamamos a `dibujar_grafica`.\n",
    "5. Nos aseguramos de cerrar el entorno al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ed3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Preparando entorno...\n",
      "Límites del estado: [-1.2  -0.07] [0.6  0.07]\n",
      "Divisiones para discretizar (bordes internos):\n",
      "  Dimensión 0: 10 cajas (10 divisiones internas)\n",
      "  Dimensión 1: 10 cajas (10 divisiones internas)\n",
      "\n",
      "2. Creando tabla Q...\n",
      "Tabla Q creada con tamaño: (10, 10, 7)\n",
      "\n",
      "3. Iniciando entrenamiento...\n",
      "\n",
      "--- Empezando Entrenamiento (6000 episodios) ---\n",
      "Episodio 100/6000 | Recompensa: -566.37 | Media (últ 100): -404.28 | Pasos: 900 | Epsilon: 0.942\n",
      "Episodio 200/6000 | Recompensa: -127.94 | Media (últ 100): -337.26 | Pasos: 900 | Epsilon: 0.887\n",
      "Episodio 300/6000 | Recompensa: -375.15 | Media (últ 100): -285.60 | Pasos: 900 | Epsilon: 0.835\n",
      "Episodio 400/6000 | Recompensa: -154.50 | Media (últ 100): -210.03 | Pasos: 900 | Epsilon: 0.787\n",
      "--- Grabando episodio 401 (Recompensa: 10.34)... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isard\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep401-reward_10_34-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep401-reward_10_34-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep401-reward_10_34-episode-0.mp4\n",
      "--- Grabación ep 401 terminada (263 pasos). ---\n",
      "Entorno de grabación para ep 401 cerrado.\n",
      "Episodio 500/6000 | Recompensa: -710.69 | Media (últ 100): -207.16 | Pasos: 900 | Epsilon: 0.741\n",
      "Episodio 600/6000 | Recompensa: -171.87 | Media (últ 100): -181.14 | Pasos: 900 | Epsilon: 0.698\n",
      "Episodio 700/6000 | Recompensa: -69.55 | Media (últ 100): -149.42 | Pasos: 900 | Epsilon: 0.657\n",
      "Episodio 800/6000 | Recompensa: -96.80 | Media (últ 100): -162.19 | Pasos: 900 | Epsilon: 0.619\n",
      "Episodio 900/6000 | Recompensa: -113.90 | Media (últ 100): -151.07 | Pasos: 900 | Epsilon: 0.583\n",
      "--- Grabando episodio 858 (Recompensa: 66.49)... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isard\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Grabación ep 858 terminada (900 pasos). ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep858-reward_66_49-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep858-reward_66_49-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep858-reward_66_49-episode-0.mp4\n",
      "Entorno de grabación para ep 858 cerrado.\n",
      "Episodio 1000/6000 | Recompensa: -66.72 | Media (últ 100): -163.56 | Pasos: 900 | Epsilon: 0.549\n",
      "Episodio 1100/6000 | Recompensa: -127.82 | Media (últ 100): -112.16 | Pasos: 900 | Epsilon: 0.517\n",
      "Episodio 1200/6000 | Recompensa: -787.06 | Media (últ 100): -125.99 | Pasos: 900 | Epsilon: 0.487\n",
      "Episodio 1300/6000 | Recompensa: -64.15 | Media (últ 100): -103.78 | Pasos: 900 | Epsilon: 0.458\n",
      "Episodio 1400/6000 | Recompensa: 54.93 | Media (últ 100): -60.05 | Pasos: 414 | Epsilon: 0.432\n",
      "--- Grabando episodio 1355 (Recompensa: 75.29)... ---\n",
      "--- Grabación ep 1355 terminada (900 pasos). ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1355-reward_75_29-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1355-reward_75_29-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1355-reward_75_29-episode-0.mp4\n",
      "Entorno de grabación para ep 1355 cerrado.\n",
      "Episodio 1500/6000 | Recompensa: -49.00 | Media (últ 100): -67.21 | Pasos: 900 | Epsilon: 0.406\n",
      "Episodio 1600/6000 | Recompensa: -8.75 | Media (últ 100): -128.58 | Pasos: 832 | Epsilon: 0.383\n",
      "Episodio 1700/6000 | Recompensa: 60.38 | Media (últ 100): -120.06 | Pasos: 544 | Epsilon: 0.360\n",
      "Episodio 1800/6000 | Recompensa: -863.93 | Media (últ 100): -121.30 | Pasos: 900 | Epsilon: 0.339\n",
      "Episodio 1900/6000 | Recompensa: 22.04 | Media (últ 100): -72.00 | Pasos: 653 | Epsilon: 0.320\n",
      "--- Grabando episodio 1946 (Recompensa: 78.81)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1946-reward_78_81-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1946-reward_78_81-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep1946-reward_78_81-episode-0.mp4\n",
      "--- Grabación ep 1946 terminada (325 pasos). ---\n",
      "Entorno de grabación para ep 1946 cerrado.\n",
      "Episodio 2000/6000 | Recompensa: 50.49 | Media (últ 100): -119.21 | Pasos: 322 | Epsilon: 0.301\n",
      "Episodio 2100/6000 | Recompensa: 65.11 | Media (últ 100): 2.24 | Pasos: 517 | Epsilon: 0.284\n",
      "Episodio 2200/6000 | Recompensa: -82.57 | Media (últ 100): -78.52 | Pasos: 900 | Epsilon: 0.267\n",
      "Episodio 2300/6000 | Recompensa: 64.62 | Media (últ 100): -39.42 | Pasos: 340 | Epsilon: 0.251\n",
      "Episodio 2400/6000 | Recompensa: -44.94 | Media (últ 100): -7.69 | Pasos: 900 | Epsilon: 0.237\n",
      "--- Grabando episodio 2407 (Recompensa: 81.13)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2407-reward_81_13-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2407-reward_81_13-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2407-reward_81_13-episode-0.mp4\n",
      "--- Grabación ep 2407 terminada (617 pasos). ---\n",
      "Entorno de grabación para ep 2407 cerrado.\n",
      "Episodio 2500/6000 | Recompensa: 42.20 | Media (últ 100): 21.52 | Pasos: 523 | Epsilon: 0.223\n",
      "Episodio 2600/6000 | Recompensa: 33.25 | Media (últ 100): -154.80 | Pasos: 255 | Epsilon: 0.210\n",
      "Episodio 2700/6000 | Recompensa: 22.20 | Media (últ 100): -43.38 | Pasos: 436 | Epsilon: 0.198\n",
      "Episodio 2800/6000 | Recompensa: 8.26 | Media (últ 100): 23.49 | Pasos: 325 | Epsilon: 0.186\n",
      "Episodio 2900/6000 | Recompensa: -125.10 | Media (últ 100): -2.84 | Pasos: 552 | Epsilon: 0.175\n",
      "--- Grabando episodio 2974 (Recompensa: 83.48)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2974-reward_83_48-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2974-reward_83_48-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep2974-reward_83_48-episode-0.mp4\n",
      "--- Grabación ep 2974 terminada (151 pasos). ---\n",
      "Entorno de grabación para ep 2974 cerrado.\n",
      "Episodio 3000/6000 | Recompensa: 69.62 | Media (últ 100): 10.31 | Pasos: 160 | Epsilon: 0.165\n",
      "Episodio 3100/6000 | Recompensa: 41.68 | Media (últ 100): 8.51 | Pasos: 346 | Epsilon: 0.156\n",
      "Episodio 3200/6000 | Recompensa: 69.24 | Media (últ 100): 35.18 | Pasos: 246 | Epsilon: 0.147\n",
      "Episodio 3300/6000 | Recompensa: -48.62 | Media (últ 100): -57.35 | Pasos: 900 | Epsilon: 0.138\n",
      "Episodio 3400/6000 | Recompensa: 66.97 | Media (últ 100): -44.67 | Pasos: 243 | Epsilon: 0.130\n",
      "--- Grabando episodio 3026 (Recompensa: 82.37)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3026-reward_82_37-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3026-reward_82_37-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3026-reward_82_37-episode-0.mp4\n",
      "--- Grabación ep 3026 terminada (165 pasos). ---\n",
      "Entorno de grabación para ep 3026 cerrado.\n",
      "Episodio 3500/6000 | Recompensa: 76.59 | Media (últ 100): 45.05 | Pasos: 162 | Epsilon: 0.122\n",
      "Episodio 3600/6000 | Recompensa: 79.47 | Media (últ 100): 35.85 | Pasos: 176 | Epsilon: 0.115\n",
      "Episodio 3700/6000 | Recompensa: 67.21 | Media (últ 100): -24.98 | Pasos: 201 | Epsilon: 0.109\n",
      "Episodio 3800/6000 | Recompensa: 67.66 | Media (últ 100): 19.75 | Pasos: 260 | Epsilon: 0.102\n",
      "Episodio 3900/6000 | Recompensa: -507.70 | Media (últ 100): -25.83 | Pasos: 894 | Epsilon: 0.096\n",
      "--- Grabando episodio 3978 (Recompensa: 83.56)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3978-reward_83_56-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3978-reward_83_56-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep3978-reward_83_56-episode-0.mp4\n",
      "--- Grabación ep 3978 terminada (274 pasos). ---\n",
      "Entorno de grabación para ep 3978 cerrado.\n",
      "Episodio 4000/6000 | Recompensa: 62.44 | Media (últ 100): -12.27 | Pasos: 194 | Epsilon: 0.091\n",
      "Episodio 4100/6000 | Recompensa: 71.43 | Media (últ 100): 51.84 | Pasos: 235 | Epsilon: 0.085\n",
      "Episodio 4200/6000 | Recompensa: 74.15 | Media (últ 100): 62.62 | Pasos: 169 | Epsilon: 0.080\n",
      "Episodio 4300/6000 | Recompensa: -125.78 | Media (últ 100): 38.74 | Pasos: 425 | Epsilon: 0.076\n",
      "Episodio 4400/6000 | Recompensa: -79.77 | Media (últ 100): -34.79 | Pasos: 407 | Epsilon: 0.071\n",
      "--- Grabando episodio 4065 (Recompensa: 83.83)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4065-reward_83_83-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4065-reward_83_83-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4065-reward_83_83-episode-0.mp4\n",
      "--- Grabación ep 4065 terminada (281 pasos). ---\n",
      "Entorno de grabación para ep 4065 cerrado.\n",
      "Episodio 4500/6000 | Recompensa: -19.36 | Media (últ 100): 69.99 | Pasos: 251 | Epsilon: 0.067\n",
      "Episodio 4600/6000 | Recompensa: 75.30 | Media (últ 100): 66.07 | Pasos: 288 | Epsilon: 0.063\n",
      "Episodio 4700/6000 | Recompensa: 71.90 | Media (últ 100): 60.80 | Pasos: 322 | Epsilon: 0.060\n",
      "Episodio 4800/6000 | Recompensa: 8.93 | Media (últ 100): 11.66 | Pasos: 294 | Epsilon: 0.056\n",
      "Episodio 4900/6000 | Recompensa: 78.73 | Media (últ 100): 36.39 | Pasos: 159 | Epsilon: 0.053\n",
      "--- Grabando episodio 4849 (Recompensa: 83.18)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4849-reward_83_18-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4849-reward_83_18-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep4849-reward_83_18-episode-0.mp4\n",
      "--- Grabación ep 4849 terminada (149 pasos). ---\n",
      "Entorno de grabación para ep 4849 cerrado.\n",
      "Episodio 5000/6000 | Recompensa: 74.90 | Media (últ 100): 3.35 | Pasos: 144 | Epsilon: 0.050\n",
      "Episodio 5100/6000 | Recompensa: 80.78 | Media (últ 100): 74.69 | Pasos: 148 | Epsilon: 0.050\n",
      "Episodio 5200/6000 | Recompensa: 78.84 | Media (últ 100): 73.33 | Pasos: 117 | Epsilon: 0.050\n",
      "Episodio 5300/6000 | Recompensa: 62.72 | Media (últ 100): 66.41 | Pasos: 141 | Epsilon: 0.050\n",
      "Episodio 5400/6000 | Recompensa: 73.66 | Media (últ 100): -37.48 | Pasos: 154 | Epsilon: 0.050\n",
      "--- Grabando episodio 5435 (Recompensa: 85.01)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5435-reward_85_01-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5435-reward_85_01-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5435-reward_85_01-episode-0.mp4\n",
      "--- Grabación ep 5435 terminada (262 pasos). ---\n",
      "Entorno de grabación para ep 5435 cerrado.\n",
      "Episodio 5500/6000 | Recompensa: 78.06 | Media (últ 100): 74.78 | Pasos: 160 | Epsilon: 0.050\n",
      "Episodio 5600/6000 | Recompensa: 82.14 | Media (últ 100): 75.01 | Pasos: 159 | Epsilon: 0.050\n",
      "Episodio 5700/6000 | Recompensa: 78.86 | Media (últ 100): 71.51 | Pasos: 165 | Epsilon: 0.050\n",
      "Episodio 5800/6000 | Recompensa: 73.53 | Media (últ 100): 11.91 | Pasos: 162 | Epsilon: 0.050\n",
      "Episodio 5900/6000 | Recompensa: 69.28 | Media (últ 100): 66.25 | Pasos: 226 | Epsilon: 0.050\n",
      "--- Grabando episodio 5718 (Recompensa: 84.40)... ---\n",
      "MoviePy - Building video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5718-reward_84_40-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5718-reward_84_40-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\isard\\Desktop\\bigData\\1CEIABDTA - 7RO\\Programación de Inteligencia Artificial\\TAREA 9 - DQN MOUNTAIN CAR CONTINUOUS\\videos\\mountaincar-mejor-ep5718-reward_84_40-episode-0.mp4\n",
      "--- Grabación ep 5718 terminada (187 pasos). ---\n",
      "Entorno de grabación para ep 5718 cerrado.\n",
      "Episodio 6000/6000 | Recompensa: 75.88 | Media (últ 100): 56.53 | Pasos: 164 | Epsilon: 0.050\n",
      "\n",
      "--- Entrenamiento Terminado ---\n",
      "\n",
      "4. Creando Gráfica de Recompensas...\n",
      "\n",
      "Gráfica guardada como: 'grafica_recompensas_montaña_notebook_ep_1_to_6000.png'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. Preparar entorno y discretización\n",
    "    print(\"\\n1. Preparando entorno...\")\n",
    "    main_env, state_bins = preparar_entorno(ENV_NAME, NUM_BINS)\n",
    "    \n",
    "    # 2. Crear tabla Q\n",
    "    print(\"\\n2. Creando tabla Q...\")\n",
    "    q_tabla_inicial = crear_q_table(NUM_BINS, NUM_ACTIONS)\n",
    "    \n",
    "    # 3. Entrenar al agente\n",
    "    print(\"\\n3. Iniciando entrenamiento...\")\n",
    "    lista_recompensas = entrenar(\n",
    "        env=main_env,\n",
    "        q_table=q_tabla_inicial, # Pasamos la tabla inicial\n",
    "        state_bins=state_bins,\n",
    "        actions=ACTIONS,\n",
    "        num_actions=NUM_ACTIONS,\n",
    "        episodes=EPISODES,\n",
    "        max_steps=MAX_STEPS,\n",
    "        lr=LR,\n",
    "        gamma=GAMMA,\n",
    "        start_epsilon=EPSILON,\n",
    "        eps_decay=EPSILON_DECAY,\n",
    "        min_epsilon=MIN_EPSILON,\n",
    "        step_penalty=STEP_PENALTY,\n",
    "        strong_action_reward=STRONG_ACTION_REWARD,\n",
    "        bottom_pos=BOTTOM_POS,                   \n",
    "        max_penalty_bottom=MAX_PENALTY_BOTTOM,       \n",
    "        pos_sensitivity=POS_SENSITIVITY, \n",
    "        vel_sensitivity=VEL_SENSITIVITY, \n",
    "        video_folder=VIDEO_FOLDER,\n",
    "        record_start=RECORD_START,\n",
    "        record_end=RECORD_END,\n",
    "        record_every=RECORD_EVERY,\n",
    "        num_bins_config=NUM_BINS\n",
    "    )\n",
    "    q_tabla_final = q_tabla_inicial # La tabla se modifica in-place\n",
    "    print(\"\\n--- Entrenamiento Terminado ---\")\n",
    "\n",
    "    # 4. Dibujar gráfica si hay recompensas\n",
    "    if lista_recompensas:\n",
    "        print(\"\\n4. Creando Gráfica de Recompensas...\")\n",
    "        dibujar_grafica(\n",
    "            recompensas=lista_recompensas,\n",
    "            start_ep=PLOT_START,\n",
    "            end_ep=PLOT_END,\n",
    "            avg_window=AVG_WINDOW,\n",
    "            filename=PLOT_NAME\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n--- No se generaron recompensas para graficar --- \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ¡ERROR FATAL! El proceso falló: {e} ---\")\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # 5. Cerrar el entorno principal\n",
    "    if main_env is not None:\n",
    "        try:\n",
    "            main_env.close()\n",
    "            print(\"\\nEntorno principal cerrado.\")\n",
    "        except Exception as close_e:\n",
    "            print(f\"Error al cerrar el entorno principal: {close_e}\")\n",
    "    \n",
    "    print(f\"\\nVídeos guardados en: '{VIDEO_FOLDER.resolve()}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
