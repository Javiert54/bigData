{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV8sjKswqmso",
        "outputId": "29a07a79-d112-4977-f185-b9d6f8245890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install gym\n",
        "# !pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAqw2Oy3-qgA",
        "outputId": "04df0c8e-5ab2-4cb2-af1d-0010afc7c586"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gym'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class MountainCarTrain:\n",
        "    def __init__(self,env):\n",
        "        self.env=env\n",
        "        self.gamma=0.99\n",
        "\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_decay = 0.05\n",
        "\n",
        "        self.epsilon_min=0.01\n",
        "\n",
        "\n",
        "        self.learingRate=0.001\n",
        "\n",
        "        self.replayBuffer=deque(maxlen=20000)\n",
        "        self.trainNetwork=self.createNetwork()\n",
        "\n",
        "        self.episodeNum=400\n",
        "\n",
        "        self.iterationNum=201 #max is 200\n",
        "\n",
        "        self.numPickFromBuffer=32\n",
        "\n",
        "        self.targetNetwork=self.createNetwork()\n",
        "\n",
        "        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n",
        "\n",
        "    def createNetwork(self):\n",
        "        model = models.Sequential()\n",
        "        state_shape = self.env.observation_space.shape\n",
        "\n",
        "        model.add(layers.Dense(24, activation='relu', input_shape=state_shape))\n",
        "        model.add(layers.Dense(48, activation='relu'))\n",
        "        model.add(layers.Dense(self.env.action_space.n,activation='linear'))\n",
        "        # model.compile(optimizer=optimizers.RMSprop(lr=self.learingRate), loss=losses.mean_squared_error)\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate =self.learingRate))\n",
        "        return model\n",
        "\n",
        "    def getBestAction(self,state):\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "\n",
        "        if np.random.rand(1) < self.epsilon:\n",
        "            action = np.random.randint(0, 3)\n",
        "        else:\n",
        "            action=np.argmax(self.trainNetwork.predict(state)[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "    def trainFromBuffer_Boost(self):\n",
        "        if len(self.replayBuffer) < self.numPickFromBuffer:\n",
        "            return\n",
        "        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n",
        "        npsamples = np.array(samples)\n",
        "        states_temp, actions_temp, rewards_temp, newstates_temp, dones_temp = np.hsplit(npsamples, 5)\n",
        "        states = np.concatenate((np.squeeze(states_temp[:])), axis = 0)\n",
        "        rewards = rewards_temp.reshape(self.numPickFromBuffer,).astype(float)\n",
        "        targets = self.trainNetwork.predict(states)\n",
        "        newstates = np.concatenate(np.concatenate(newstates_temp))\n",
        "        dones = np.concatenate(dones_temp).astype(bool)\n",
        "        notdones = ~dones\n",
        "        notdones = notdones.astype(float)\n",
        "        dones = dones.astype(float)\n",
        "        Q_futures = self.targetNetwork.predict(newstates).max(axis = 1)\n",
        "        targets[(np.arange(self.numPickFromBuffer), actions_temp.reshape(self.numPickFromBuffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n",
        "        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "    def trainFromBuffer(self):\n",
        "        if len(self.replayBuffer) < self.numPickFromBuffer:\n",
        "            return\n",
        "\n",
        "        samples = random.sample(self.replayBuffer,self.numPickFromBuffer)\n",
        "\n",
        "        states = []\n",
        "        newStates=[]\n",
        "        for sample in samples:\n",
        "            state, action, reward, new_state, done = sample\n",
        "            states.append(state)\n",
        "            newStates.append(new_state)\n",
        "\n",
        "        newArray = np.array(states)\n",
        "        states = newArray.reshape(self.numPickFromBuffer, 2)\n",
        "\n",
        "        newArray2 = np.array(newStates)\n",
        "        newStates = newArray2.reshape(self.numPickFromBuffer, 2)\n",
        "\n",
        "        targets = self.trainNetwork.predict(states)\n",
        "        new_state_targets=self.targetNetwork.predict(newStates)\n",
        "\n",
        "        i=0\n",
        "        for sample in samples:\n",
        "            state, action, reward, new_state, done = sample\n",
        "            target = targets[i]\n",
        "            if done:\n",
        "                target[action] = reward\n",
        "            else:\n",
        "                Q_future = max(new_state_targets[i])\n",
        "                target[action] = reward + Q_future * self.gamma\n",
        "            i+=1\n",
        "\n",
        "        self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "    def orginalTry(self,currentState,eps):\n",
        "        rewardSum = 0\n",
        "        max_position=-99\n",
        "\n",
        "        for i in range(self.iterationNum):\n",
        "            bestAction = self.getBestAction(currentState)\n",
        "\n",
        "            #show the animation every 50 eps\n",
        "            if eps%50==0:\n",
        "                env.render()\n",
        "\n",
        "            # Modify the line causing the error:\n",
        "            step_result = env.step(bestAction)  # Get the complete result\n",
        "\n",
        "            # Adapt to the structure of the returned value, handle potential additional elements\n",
        "            # Assuming the first 4 elements are still state, reward, done, and info\n",
        "            new_state, reward, done, *_ = step_result\n",
        "\n",
        "\n",
        "            new_state = new_state.reshape(1, 2)\n",
        "\n",
        "            # # Keep track of max position\n",
        "            if new_state[0][0] > max_position:\n",
        "                max_position = new_state[0][0]\n",
        "\n",
        "\n",
        "            # # Adjust reward for task completion\n",
        "            if new_state[0][0] >= 0.5:\n",
        "                reward += 10\n",
        "\n",
        "            self.replayBuffer.append([currentState, bestAction, reward, new_state, done])\n",
        "\n",
        "            #Or you can use self.trainFromBuffer_Boost(), it is a matrix wise version for boosting\n",
        "            self.trainFromBuffer()\n",
        "\n",
        "            rewardSum += reward\n",
        "\n",
        "            currentState = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    def start(self):\n",
        "            for eps in range(self.episodeNum):\n",
        "                currentState, info = env.reset()  # Get both observation and info\n",
        "                currentState = currentState.reshape(1, 2)  # Reshape the observation\n",
        "                self.orginalTry(currentState, eps)\n",
        "\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "dqn=MountainCarTrain(env=env)\n",
        "dqn.start()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
