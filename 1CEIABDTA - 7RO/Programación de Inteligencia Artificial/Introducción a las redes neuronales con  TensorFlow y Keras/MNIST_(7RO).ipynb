{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import Zeros\n",
        "# Generar el dataset (peso, altura, IMC)\n",
        "np.random.seed(42)\n",
        "def generar_dataset(num_filas):\n",
        "    alturas = np.round(np.random.uniform(1.4, 2.0, num_filas), 2)\n",
        "    pesos = np.round(np.random.uniform(40, 120, num_filas), 1)\n",
        "    imc = np.round(pesos / (alturas ** 2), 2)\n",
        "    return pd.DataFrame({'peso': pesos, 'altura': alturas, 'IMC': imc})\n",
        "\n",
        "# Crear dataset de 1000 filas\n",
        "dataset = generar_dataset(1000)\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X = dataset[['peso', 'altura']].values  # Entradas: peso y altura\n",
        "y = dataset['IMC'].values  # Salida: IMC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar las entradas\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir la arquitectura de la red neuronal\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', kernel_initializer=Zeros(), input_shape=(2,)),\n",
        "    Dense(32, activation='relu', kernel_initializer=Zeros()),\n",
        "    Dense(1, kernel_initializer=Zeros())\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Pérdida (MSE): {loss:.4f}, Error Absoluto Medio (MAE): {mae:.4f}\")\n",
        "\n",
        "# Realizar predicciones\n",
        "nuevos_datos = np.array([[70, 1.75], [90, 1.8]])  # Ejemplo de peso y altura\n",
        "nuevos_datos = scaler.transform(nuevos_datos)\n",
        "predicciones = model.predict(nuevos_datos)\n",
        "\n",
        "print(\"Predicción de IMC para nuevos datos:\")\n",
        "for i, imc_pred in enumerate(predicciones):\n",
        "    print(f\"Peso: {nuevos_datos[i][0]:.2f}, Altura: {nuevos_datos[i][1]:.2f}, IMC Predicho: {imc_pred[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F50n__wDOrB",
        "outputId": "07282b82-4fed-4d9d-8c79-c41794a4aa96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 975.1495 - mae: 29.3557 - val_loss: 915.5813 - val_mae: 28.4608\n",
            "Epoch 2/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 935.3278 - mae: 28.7010 - val_loss: 912.7529 - val_mae: 28.4111\n",
            "Epoch 3/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 920.4978 - mae: 28.6185 - val_loss: 909.9437 - val_mae: 28.3616\n",
            "Epoch 4/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 945.4979 - mae: 28.9449 - val_loss: 907.1130 - val_mae: 28.3116\n",
            "Epoch 5/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 927.6852 - mae: 28.6152 - val_loss: 904.3033 - val_mae: 28.2620\n",
            "Epoch 6/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 933.2714 - mae: 28.7686 - val_loss: 901.5087 - val_mae: 28.2125\n",
            "Epoch 7/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 926.7836 - mae: 28.6576 - val_loss: 898.7144 - val_mae: 28.1629\n",
            "Epoch 8/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 939.5685 - mae: 28.7021 - val_loss: 895.9205 - val_mae: 28.1133\n",
            "Epoch 9/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 922.6476 - mae: 28.6089 - val_loss: 893.1481 - val_mae: 28.0639\n",
            "Epoch 10/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 908.2949 - mae: 28.2896 - val_loss: 890.3677 - val_mae: 28.0143\n",
            "Epoch 11/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 951.6160 - mae: 28.9928 - val_loss: 887.6091 - val_mae: 27.9651\n",
            "Epoch 12/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 949.3222 - mae: 28.9401 - val_loss: 884.8488 - val_mae: 27.9157\n",
            "Epoch 13/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 915.9038 - mae: 28.3337 - val_loss: 882.1058 - val_mae: 27.8665\n",
            "Epoch 14/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 934.0199 - mae: 28.6286 - val_loss: 879.3442 - val_mae: 27.8169\n",
            "Epoch 15/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 907.5147 - mae: 28.2987 - val_loss: 876.6071 - val_mae: 27.7677\n",
            "Epoch 16/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 911.5829 - mae: 28.3546 - val_loss: 873.8781 - val_mae: 27.7185\n",
            "Epoch 17/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 895.5925 - mae: 28.0671 - val_loss: 871.1683 - val_mae: 27.6696\n",
            "Epoch 18/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 872.1006 - mae: 27.7264 - val_loss: 868.4582 - val_mae: 27.6205\n",
            "Epoch 19/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 899.7432 - mae: 28.1422 - val_loss: 865.7316 - val_mae: 27.5711\n",
            "Epoch 20/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 878.5395 - mae: 27.7821 - val_loss: 863.0356 - val_mae: 27.5222\n",
            "Epoch 21/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 864.5588 - mae: 27.5257 - val_loss: 860.3174 - val_mae: 27.4728\n",
            "Epoch 22/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 868.4568 - mae: 27.5988 - val_loss: 857.6312 - val_mae: 27.4238\n",
            "Epoch 23/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 864.0751 - mae: 27.4803 - val_loss: 854.9529 - val_mae: 27.3750\n",
            "Epoch 24/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 878.8776 - mae: 27.6461 - val_loss: 852.2709 - val_mae: 27.3259\n",
            "Epoch 25/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 888.0542 - mae: 27.9530 - val_loss: 849.5910 - val_mae: 27.2769\n",
            "Epoch 26/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 899.0553 - mae: 28.0878 - val_loss: 846.9252 - val_mae: 27.2279\n",
            "Epoch 27/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 873.9813 - mae: 27.6656 - val_loss: 844.2664 - val_mae: 27.1791\n",
            "Epoch 28/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 870.9589 - mae: 27.6886 - val_loss: 841.6169 - val_mae: 27.1303\n",
            "Epoch 29/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 939.9677 - mae: 28.6827 - val_loss: 838.9534 - val_mae: 27.0812\n",
            "Epoch 30/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 813.4029 - mae: 26.6360 - val_loss: 836.3376 - val_mae: 27.0328\n",
            "Epoch 31/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 852.6218 - mae: 27.3932 - val_loss: 833.6912 - val_mae: 26.9838\n",
            "Epoch 32/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 872.2575 - mae: 27.5667 - val_loss: 831.0559 - val_mae: 26.9350\n",
            "Epoch 33/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 848.3061 - mae: 27.2255 - val_loss: 828.4243 - val_mae: 26.8861\n",
            "Epoch 34/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 857.9965 - mae: 27.4398 - val_loss: 825.8020 - val_mae: 26.8372\n",
            "Epoch 35/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 890.2758 - mae: 27.8803 - val_loss: 823.2012 - val_mae: 26.7888\n",
            "Epoch 36/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 843.0283 - mae: 27.1317 - val_loss: 820.5928 - val_mae: 26.7400\n",
            "Epoch 37/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 855.4870 - mae: 27.3392 - val_loss: 817.9955 - val_mae: 26.6914\n",
            "Epoch 38/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 841.0569 - mae: 27.0582 - val_loss: 815.4062 - val_mae: 26.6429\n",
            "Epoch 39/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 859.6757 - mae: 27.3572 - val_loss: 812.8101 - val_mae: 26.5941\n",
            "Epoch 40/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 821.6764 - mae: 26.7893 - val_loss: 810.2363 - val_mae: 26.5457\n",
            "Epoch 41/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 846.5320 - mae: 27.2256 - val_loss: 807.6481 - val_mae: 26.4969\n",
            "Epoch 42/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 870.3639 - mae: 27.5130 - val_loss: 805.0809 - val_mae: 26.4484\n",
            "Epoch 43/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 844.1315 - mae: 27.1290 - val_loss: 802.5151 - val_mae: 26.3998\n",
            "Epoch 44/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 813.5184 - mae: 26.5806 - val_loss: 799.9687 - val_mae: 26.3516\n",
            "Epoch 45/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 832.2335 - mae: 26.9466 - val_loss: 797.4001 - val_mae: 26.3028\n",
            "Epoch 46/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 834.2390 - mae: 27.0050 - val_loss: 794.8588 - val_mae: 26.2544\n",
            "Epoch 47/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 839.5218 - mae: 26.9895 - val_loss: 792.3127 - val_mae: 26.2059\n",
            "Epoch 48/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 841.5446 - mae: 27.0507 - val_loss: 789.7949 - val_mae: 26.1578\n",
            "Epoch 49/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 806.7000 - mae: 26.6344 - val_loss: 787.2480 - val_mae: 26.1091\n",
            "Epoch 50/50\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 799.0316 - mae: 26.4167 - val_loss: 784.7228 - val_mae: 26.0607\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 804.3414 - mae: 26.3409 \n",
            "Pérdida (MSE): 784.7229, Error Absoluto Medio (MAE): 26.0607\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Predicción de IMC para nuevos datos:\n",
            "Peso: -0.48, Altura: 0.30, IMC Predicho: 2.45\n",
            "Peso: 0.38, Altura: 0.58, IMC Predicho: 2.45\n"
          ]
        }
      ]
    }
  ]
}