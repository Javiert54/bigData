{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>297.572857</td>\n",
       "      <td>...</td>\n",
       "      <td>32.00</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>298.211429</td>\n",
       "      <td>...</td>\n",
       "      <td>17.94</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>298.781429</td>\n",
       "      <td>...</td>\n",
       "      <td>26.10</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>298.987143</td>\n",
       "      <td>...</td>\n",
       "      <td>13.90</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>299.518571</td>\n",
       "      <td>...</td>\n",
       "      <td>12.20</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
       "1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
       "2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
       "3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
       "4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
       "\n",
       "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "0  0.177617                 12.42             297.572857  ...   \n",
       "1  0.155486                 22.82             298.211429  ...   \n",
       "2  0.170843                 34.54             298.781429  ...   \n",
       "3  0.235886                 15.36             298.987143  ...   \n",
       "4  0.247340                  7.52             299.518571  ...   \n",
       "\n",
       "   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
       "0                            32.00                             73.365714   \n",
       "1                            17.94                             77.368571   \n",
       "2                            26.10                             82.052857   \n",
       "3                            13.90                             80.337143   \n",
       "4                            12.20                             80.460000   \n",
       "\n",
       "   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
       "0                         12.42                              14.012857   \n",
       "1                         22.82                              15.372857   \n",
       "2                         34.54                              16.848571   \n",
       "3                         15.36                              16.672857   \n",
       "4                          7.52                              17.210000   \n",
       "\n",
       "   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
       "0           2.628571           25.442857                 6.900000   \n",
       "1           2.371429           26.714286                 6.371429   \n",
       "2           2.300000           26.714286                 6.485714   \n",
       "3           2.428571           27.471429                 6.771429   \n",
       "4           3.014286           28.942857                 9.371429   \n",
       "\n",
       "   station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
       "0                29.4                20.0               16.0  \n",
       "1                31.7                22.2                8.6  \n",
       "2                32.2                22.8               41.4  \n",
       "3                33.3                23.3                4.0  \n",
       "4                35.0                23.9                5.8  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importar el dataset desde un archivo CSV\n",
    "df_train = pd.read_csv('dengue_features_train.csv', sep=',', header=0)\n",
    "\n",
    "# Mostrar las primeras filas del dataframe\n",
    "df_train.head()\n",
    "\n",
    "# df_labels = pd.read_csv('dengue_labels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando GridSearchCV para optimización de hiperparámetros...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Mejores parámetros encontrados: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "Entrenando RandomForestRegressor con los mejores parámetros...\n",
      "\n",
      "Mean Absolute Error (MAE): 13.2808\n",
      "\n",
      "Contexto para el MAE (Estadísticas de 'total_cases' en el conjunto de prueba):\n",
      "- Media:      25.4829\n",
      "- Mediana:    13.0000\n",
      "- Desv. Est.: 41.7328\n",
      "- Mínimo:     0.0000\n",
      "- Máximo:     410.0000\n",
      "\n",
      "- MAE como % de la Media: 52.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2892: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sys\n",
    "\n",
    "def load_and_merge_data(features_path, labels_path):\n",
    "    \"\"\"\n",
    "    Carga los datasets de características y etiquetas, y los une.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        features_df = pd.read_csv(features_path, parse_dates=['week_start_date'])\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "        df = pd.merge(features_df, labels_df, on=['city', 'year', 'weekofyear'])\n",
    "        df.sort_values(by=['city', 'week_start_date'], inplace=True)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontraron los archivos en las rutas especificadas:\")\n",
    "        print(f\"- {features_path}\")\n",
    "        print(f\"- {labels_path}\")\n",
    "        return None\n",
    "\n",
    "def create_preprocessor(df):\n",
    "    \"\"\"\n",
    "    Crea un preprocesador para manejar características categóricas y numéricas.\n",
    "    \"\"\"\n",
    "    # Seleccionar columnas numéricas (excluyendo las que no son features y la variable objetivo)\n",
    "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    numeric_features = [col for col in numeric_features if col not in ['year', 'weekofyear', 'total_cases']]\n",
    "    \n",
    "    # Variables categóricas\n",
    "    categorical_features = ['city']\n",
    "    \n",
    "    # Pipeline para variables numéricas: imputación por mediana y escalado\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Pipeline para variables categóricas: OneHotEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'  # Incluye las columnas que no se especificaron\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def preprocess_and_split(df, preprocessor, target_col='total_cases', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos y los divide respetando la transformación de la variable objetivo.\n",
    "    \"\"\"\n",
    "    # Extraer variables temporales a partir de 'week_start_date'\n",
    "    df['month'] = df['week_start_date'].dt.month\n",
    "    df['day'] = df['week_start_date'].dt.day\n",
    "    df['day_of_week'] = df['week_start_date'].dt.dayofweek\n",
    "    \n",
    "    # Transformar la variable objetivo para mitigar el sesgo en la distribución.\n",
    "    # Se utiliza log1p para poder trabajar con ceros.\n",
    "    y = np.log1p(df[target_col])\n",
    "    \n",
    "    # Eliminar columnas no deseadas para el modelo\n",
    "    X = df.drop(columns=[target_col, 'week_start_date'])\n",
    "    \n",
    "    # Dividir en conjunto de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Ajustar el preprocesador con X_train y transformar ambos conjuntos\n",
    "    preprocessor.fit(X_train)\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Verificar si quedan NaNs tras el preprocesamiento\n",
    "    if np.isnan(X_train_processed).sum() > 0 or np.isnan(X_test_processed).sum() > 0:\n",
    "         print(\"Advertencia: NaNs detectados después del preprocesamiento.\")\n",
    "    \n",
    "    return X_train_processed, X_test_processed, y_train, y_test\n",
    "\n",
    "def train_predict_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entrena el modelo, realiza la predicción y evalúa el rendimiento utilizando GridSearchCV.\n",
    "    \"\"\"\n",
    "    # Definir la rejilla de hiperparámetros a explorar\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_leaf': [1, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model, \n",
    "        param_grid=param_grid,\n",
    "        cv=3, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Iniciando GridSearchCV para optimización de hiperparámetros...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
    "    \n",
    "    model = grid_search.best_estimator_\n",
    "    \n",
    "    print(\"Entrenando RandomForestRegressor con los mejores parámetros...\")\n",
    "    # El modelo ya se ha entrenado durante el GridSearchCV\n",
    "    predictions_log = model.predict(X_test)\n",
    "    \n",
    "    # Invertir la transformación logarítmica para obtener predicciones en la escala original\n",
    "    predictions = np.expm1(predictions_log)\n",
    "    predictions = np.maximum(0, predictions.round().astype(int))\n",
    "    y_test_original = np.expm1(y_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_original, predictions)\n",
    "    print(f'\\nMean Absolute Error (MAE): {mae:.4f}')\n",
    "    \n",
    "    print(\"\\nContexto para el MAE (Estadísticas de 'total_cases' en el conjunto de prueba):\")\n",
    "    stats = pd.Series(y_test_original).describe()\n",
    "    print(f\"- Media:      {stats['mean']:.4f}\")\n",
    "    print(f\"- Mediana:    {stats['50%']:.4f}\")\n",
    "    print(f\"- Desv. Est.: {stats['std']:.4f}\")\n",
    "    print(f\"- Mínimo:     {stats['min']:.4f}\")\n",
    "    print(f\"- Máximo:     {stats['max']:.4f}\")\n",
    "    \n",
    "    if stats['mean'] != 0:\n",
    "        mae_perc_mean = (mae / stats['mean']) * 100\n",
    "        print(f\"\\n- MAE como % de la Media: {mae_perc_mean:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n- No se puede calcular MAE como % de la Media (la media es 0).\")\n",
    "\n",
    "# --- Flujo Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    FEATURES_FILE = 'dengue_features_train.csv'\n",
    "    LABELS_FILE = 'dengue_labels_train.csv'\n",
    "    \n",
    "    # 1. Cargar y unir los datos\n",
    "    merged_df = load_and_merge_data(FEATURES_FILE, LABELS_FILE)\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        # 2. Crear el preprocesador basado en el DataFrame completo (sin la variable objetivo y fecha)\n",
    "        preprocessor = create_preprocessor(merged_df.drop(columns=['total_cases', 'week_start_date']))\n",
    "        \n",
    "        # 3. Preprocesar y dividir los datos\n",
    "        X_train, X_test, y_train, y_test = preprocess_and_split(merged_df, preprocessor)\n",
    "        \n",
    "        # 4. Entrenar, predecir y evaluar el modelo\n",
    "        train_predict_evaluate(X_train, X_test, y_train, y_test)\n",
    "    else:\n",
    "        print(\"Proceso detenido debido a error en la carga de archivos.\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
