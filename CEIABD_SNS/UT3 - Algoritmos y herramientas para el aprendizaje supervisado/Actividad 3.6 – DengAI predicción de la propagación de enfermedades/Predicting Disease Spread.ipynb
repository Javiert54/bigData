{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import all necessary libraries, including pandas, numpy, sklearn, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utility Functions\n",
    "Define the functions `load_and_merge_data`, `create_preprocessor`, `preprocess_and_split`, and `train_predict_evaluate` in separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `load_and_merge_data` function\n",
    "def load_and_merge_data(features_path, labels_path):\n",
    "    \"\"\"\n",
    "    Carga los datasets de características y etiquetas, y los une en un único DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        features_df = pd.read_csv(features_path, parse_dates=['week_start_date'])\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "        df = pd.merge(features_df, labels_df, on=['city', 'year', 'weekofyear'])\n",
    "        df.sort_values(by=['city', 'week_start_date'], inplace=True)\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the `create_preprocessor` function\n",
    "def create_preprocessor(df):\n",
    "    \"\"\"\n",
    "    Crea un preprocesador para manejar características categóricas y numéricas.\n",
    "    \"\"\"\n",
    "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    numeric_features = [col for col in numeric_features if col not in ['year', 'weekofyear', 'total_cases']]\n",
    "    categorical_features = ['city']\n",
    "\n",
    "    # Pipelines para transformar datos numéricos y categóricos\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # ColumnTransformer para aplicar las transformaciones\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "# Define the `preprocess_and_split` function\n",
    "def preprocess_and_split(df, preprocessor, target_col='total_cases', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos, extrae características temporales y divide en conjuntos de entrenamiento y prueba.\n",
    "    \"\"\"\n",
    "    # Extraer características temporales\n",
    "    df['month'] = df['week_start_date'].dt.month\n",
    "    df['day'] = df['week_start_date'].dt.day\n",
    "    df['day_of_week'] = df['week_start_date'].dt.dayofweek\n",
    "\n",
    "    # Transformar la variable objetivo con log1p\n",
    "    y = np.log1p(df[target_col])\n",
    "    X = df.drop(columns=[target_col, 'week_start_date'])\n",
    "\n",
    "    # Dividir en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Ajustar y transformar los datos con el preprocesador\n",
    "    preprocessor.fit(X_train)\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train_processed, X_test_processed, y_train, y_test\n",
    "\n",
    "# Define the `train_predict_evaluate` function\n",
    "def train_predict_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo RandomForestRegressor, realiza predicciones y evalúa el rendimiento.\n",
    "    \"\"\"\n",
    "    # Definir hiperparámetros para GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_leaf': [1, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Configurar GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model = grid_search.best_estimator_\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions_log = model.predict(X_test)\n",
    "    predictions = np.maximum(0, np.expm1(predictions_log).round().astype(int))\n",
    "    y_test_original = np.expm1(y_test)\n",
    "\n",
    "    # Calcular y mostrar el MAE\n",
    "    mae = mean_absolute_error(y_test_original, predictions)\n",
    "    print(f'\\nMean Absolute Error (MAE): {mae:.4f}')\n",
    "\n",
    "    # Mostrar estadísticas del conjunto de prueba\n",
    "    stats = pd.Series(y_test_original).describe()\n",
    "    print(f\"\\nEstadísticas de 'total_cases' en el conjunto de prueba:\")\n",
    "    print(stats)\n",
    "    print(f\"\\nMAE como % de la Media: {(mae / stats['mean']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Load and Merge Data\n",
    "Load the datasets using the `load_and_merge_data` function and display the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>iq</td>\n",
       "      <td>2000</td>\n",
       "      <td>26</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>0.192886</td>\n",
       "      <td>0.132257</td>\n",
       "      <td>0.340886</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>25.41</td>\n",
       "      <td>296.740000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.418571</td>\n",
       "      <td>25.41</td>\n",
       "      <td>16.651429</td>\n",
       "      <td>8.928571</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>10.775000</td>\n",
       "      <td>32.5</td>\n",
       "      <td>20.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>iq</td>\n",
       "      <td>2000</td>\n",
       "      <td>27</td>\n",
       "      <td>2000-07-08</td>\n",
       "      <td>0.216833</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.289457</td>\n",
       "      <td>0.241657</td>\n",
       "      <td>60.61</td>\n",
       "      <td>296.634286</td>\n",
       "      <td>...</td>\n",
       "      <td>93.581429</td>\n",
       "      <td>60.61</td>\n",
       "      <td>16.862857</td>\n",
       "      <td>10.314286</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>11.566667</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>55.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>iq</td>\n",
       "      <td>2000</td>\n",
       "      <td>28</td>\n",
       "      <td>2000-07-15</td>\n",
       "      <td>0.176757</td>\n",
       "      <td>0.173129</td>\n",
       "      <td>0.204114</td>\n",
       "      <td>0.128014</td>\n",
       "      <td>55.52</td>\n",
       "      <td>296.415714</td>\n",
       "      <td>...</td>\n",
       "      <td>95.848571</td>\n",
       "      <td>55.52</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>7.385714</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>38.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>iq</td>\n",
       "      <td>2000</td>\n",
       "      <td>29</td>\n",
       "      <td>2000-07-22</td>\n",
       "      <td>0.227729</td>\n",
       "      <td>0.145429</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.200314</td>\n",
       "      <td>5.60</td>\n",
       "      <td>295.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>87.234286</td>\n",
       "      <td>5.60</td>\n",
       "      <td>14.431429</td>\n",
       "      <td>9.114286</td>\n",
       "      <td>25.766667</td>\n",
       "      <td>10.533333</td>\n",
       "      <td>31.5</td>\n",
       "      <td>14.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>iq</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>2000-07-29</td>\n",
       "      <td>0.328643</td>\n",
       "      <td>0.322129</td>\n",
       "      <td>0.254371</td>\n",
       "      <td>0.361043</td>\n",
       "      <td>62.76</td>\n",
       "      <td>296.432857</td>\n",
       "      <td>...</td>\n",
       "      <td>88.161429</td>\n",
       "      <td>62.76</td>\n",
       "      <td>15.444286</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>11.480000</td>\n",
       "      <td>33.3</td>\n",
       "      <td>19.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "936   iq  2000          26      2000-07-01  0.192886  0.132257  0.340886   \n",
       "937   iq  2000          27      2000-07-08  0.216833  0.276100  0.289457   \n",
       "938   iq  2000          28      2000-07-15  0.176757  0.173129  0.204114   \n",
       "939   iq  2000          29      2000-07-22  0.227729  0.145429  0.254200   \n",
       "940   iq  2000          30      2000-07-29  0.328643  0.322129  0.254371   \n",
       "\n",
       "      ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "936  0.247200                 25.41             296.740000  ...   \n",
       "937  0.241657                 60.61             296.634286  ...   \n",
       "938  0.128014                 55.52             296.415714  ...   \n",
       "939  0.200314                  5.60             295.357143  ...   \n",
       "940  0.361043                 62.76             296.432857  ...   \n",
       "\n",
       "     reanalysis_relative_humidity_percent  reanalysis_sat_precip_amt_mm  \\\n",
       "936                             92.418571                         25.41   \n",
       "937                             93.581429                         60.61   \n",
       "938                             95.848571                         55.52   \n",
       "939                             87.234286                          5.60   \n",
       "940                             88.161429                         62.76   \n",
       "\n",
       "     reanalysis_specific_humidity_g_per_kg  reanalysis_tdtr_k  \\\n",
       "936                              16.651429           8.928571   \n",
       "937                              16.862857          10.314286   \n",
       "938                              17.120000           7.385714   \n",
       "939                              14.431429           9.114286   \n",
       "940                              15.444286           9.500000   \n",
       "\n",
       "     station_avg_temp_c  station_diur_temp_rng_c  station_max_temp_c  \\\n",
       "936           26.400000                10.775000                32.5   \n",
       "937           26.900000                11.566667                34.0   \n",
       "938           26.800000                11.466667                33.0   \n",
       "939           25.766667                10.533333                31.5   \n",
       "940           26.600000                11.480000                33.3   \n",
       "\n",
       "     station_min_temp_c  station_precip_mm  total_cases  \n",
       "936                20.7                3.0            0  \n",
       "937                20.8               55.6            0  \n",
       "938                20.7               38.1            0  \n",
       "939                14.7               30.0            0  \n",
       "940                19.1                4.0            0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and merge the datasets\n",
    "FEATURES_FILE = 'dengue_features_train.csv'\n",
    "LABELS_FILE = 'dengue_labels_train.csv'\n",
    "\n",
    "merged_df = load_and_merge_data(FEATURES_FILE, LABELS_FILE)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "if merged_df is not None:\n",
    "    display(merged_df.head())\n",
    "else:\n",
    "    print(\"Error in loading data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Create Preprocessor\n",
    "Create the preprocessor using the `create_preprocessor` function and display its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw',\n",
      "                                  'precipitation_amt_mm',\n",
      "                                  'reanalysis_air_temp_k',\n",
      "                                  'reanalysis_avg_temp_k',\n",
      "                                  'reanalysis_dew_point_temp_k',\n",
      "                                  'reanalysis_max_air_temp_k',\n",
      "                                  'reanalysis_min_air_temp_k',\n",
      "                                  'reanalysis_precip_amt_kg_per_m2',\n",
      "                                  'reanalysis_relative_humidity_percent',\n",
      "                                  'reanalysis_sat_precip_amt_mm',\n",
      "                                  'reanalysis_specific_humidity_g_per_kg',\n",
      "                                  'reanalysis_tdtr_k', 'station_avg_temp_c',\n",
      "                                  'station_diur_temp_rng_c',\n",
      "                                  'station_max_temp_c', 'station_min_temp_c',\n",
      "                                  'station_precip_mm']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['city'])])\n"
     ]
    }
   ],
   "source": [
    "# Create the preprocessor using the `create_preprocessor` function\n",
    "if merged_df is not None:\n",
    "    preprocessor = create_preprocessor(merged_df.drop(columns=['total_cases', 'week_start_date']))\n",
    "    # Display the preprocessor configuration\n",
    "    print(preprocessor)\n",
    "else:\n",
    "    print(\"Merged DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Preprocess and Split Data\n",
    "Preprocess the data and split it into training and testing sets using the `preprocess_and_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1164, 27)\n",
      "X_test shape: (292, 27)\n",
      "y_train shape: (1164,)\n",
      "y_test shape: (292,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and split the data\n",
    "if merged_df is not None:\n",
    "    X_train, X_test, y_train, y_test = preprocess_and_split(merged_df, preprocessor)\n",
    "    # Display the shapes of the processed datasets\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"Merged DataFrame is not available for preprocessing and splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Train, Predict, and Evaluate\n",
    "Train the model, make predictions, and evaluate its performance using the `train_predict_evaluate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "\n",
      "Mean Absolute Error (MAE): 13.2808\n",
      "\n",
      "Estadísticas de 'total_cases' en el conjunto de prueba:\n",
      "count    292.000000\n",
      "mean      25.482877\n",
      "std       41.732783\n",
      "min        0.000000\n",
      "25%        5.000000\n",
      "50%       13.000000\n",
      "75%       28.250000\n",
      "max      410.000000\n",
      "Name: total_cases, dtype: float64\n",
      "\n",
      "MAE como % de la Media: 52.12%\n"
     ]
    }
   ],
   "source": [
    "# Train, Predict, and Evaluate\n",
    "if merged_df is not None:\n",
    "    train_predict_evaluate(X_train, X_test, y_train, y_test)\n",
    "else:\n",
    "    print(\"Data is not available for training, prediction, and evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
