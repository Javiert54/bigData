{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import all necessary libraries, including pandas, numpy, sklearn, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV # Añadir RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor # Añadir KNeighborsRegressor\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Utilities\n",
    "import sys\n",
    "from scipy.stats import randint # Para RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utility Functions\n",
    "Define the functions `load_and_merge_data`, `create_preprocessor`, `preprocess_and_split`, and `train_predict_evaluate` in separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `load_and_merge_data` function (sin cambios)\n",
    "def load_and_merge_data(features_path, labels_path):\n",
    "    \"\"\"\n",
    "    Carga los datasets de características y etiquetas, y los une en un único DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        features_df = pd.read_csv(features_path, parse_dates=['week_start_date'])\n",
    "        labels_df = pd.read_csv(labels_path)\n",
    "        df = pd.merge(features_df, labels_df, on=['city', 'year', 'weekofyear'])\n",
    "        # Ordenar cronológicamente puede ser mejor para evaluación, pero mantenemos el original por ahora\n",
    "        df.sort_values(by=['city', 'week_start_date'], inplace=True)\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the `create_preprocessor` function (sin cambios)\n",
    "def create_preprocessor(df):\n",
    "    \"\"\"\n",
    "    Crea un preprocesador para manejar características categóricas y numéricas.\n",
    "    Incluye StandardScaler, bueno para KNN.\n",
    "    \"\"\"\n",
    "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Excluir columnas que no son features directas o el target si estuviera\n",
    "    numeric_features = [col for col in numeric_features if col not in ['year', 'weekofyear', 'total_cases']]\n",
    "    categorical_features = ['city']\n",
    "\n",
    "    # Pipelines para transformar datos numéricos y categóricos\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')), # Usar mediana es más robusto\n",
    "        ('scaler', StandardScaler()) # Importante para KNN\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse_output=False para facilitar manejo\n",
    "    ])\n",
    "\n",
    "    # ColumnTransformer para aplicar las transformaciones\n",
    "    # Usar set_output para que devuelva DataFrames\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough', # Mantiene columnas no especificadas (ej. month, day, day_of_week)\n",
    "        verbose_feature_names_out=False # Nombres de columnas más limpios\n",
    "    )\n",
    "    preprocessor.set_output(transform=\"pandas\")\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# Define the `preprocess_and_split` function\n",
    "def preprocess_and_split(df, preprocessor, target_col='total_cases', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos, extrae características temporales y divide en conjuntos de entrenamiento y prueba.\n",
    "    \"\"\"\n",
    "    # --- Debugging Prints Start ---\n",
    "    print(f\"--- Inside preprocess_and_split ---\")\n",
    "    print(f\"Value of target_col at entry: {target_col}\")\n",
    "    print(f\"Type of target_col at entry: {type(target_col)}\")\n",
    "    # --- Debugging Prints End ---\n",
    "\n",
    "    # Extraer características temporales\n",
    "    df = df.copy() # Good practice\n",
    "    df['month'] = df['week_start_date'].dt.month\n",
    "    df['day'] = df['week_start_date'].dt.day\n",
    "    df['day_of_week'] = df['week_start_date'].dt.dayofweek\n",
    "\n",
    "    # Transformar la variable objetivo con log1p\n",
    "    print(f\"Attempting to access df column using target_col: '{target_col}'\") # Debug print\n",
    "    try:\n",
    "        y = np.log1p(df[target_col])\n",
    "    except KeyError as e:\n",
    "        print(f\"ERROR: KeyError occurred when accessing df[target_col].\")\n",
    "        print(f\"       Value of target_col was: {target_col}\")\n",
    "        print(f\"       Type of target_col was: {type(target_col)}\")\n",
    "        print(f\"       Columns available in df: {df.columns.tolist()}\")\n",
    "        raise e # Re-raise the error after printing details\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An unexpected error occurred: {e}\")\n",
    "        raise e\n",
    "\n",
    "    X = df.drop(columns=[target_col, 'week_start_date'])\n",
    "\n",
    "    # Dividir en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Ajustar y transformar los datos con el preprocesador\n",
    "    # --- Optional: Use set_output for pandas DataFrame output ---\n",
    "    if hasattr(preprocessor, 'set_output'):\n",
    "        try:\n",
    "            preprocessor.set_output(transform=\"pandas\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: Could not set preprocessor output to pandas: {e}\")\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    print(\"Fitting preprocessor...\") # Debug print\n",
    "    preprocessor.fit(X_train)\n",
    "    print(\"Transforming train data...\") # Debug print\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    print(\"Transforming test data...\") # Debug print\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # --- Optional: Verification for NaNs after processing ---\n",
    "    if isinstance(X_train_processed, pd.DataFrame) and X_train_processed.isnull().sum().sum() > 0:\n",
    "         print(\"Advertencia: NaNs detectados en X_train DESPUÉS del preprocesamiento.\")\n",
    "    if isinstance(X_test_processed, pd.DataFrame) and X_test_processed.isnull().sum().sum() > 0:\n",
    "         print(\"Advertencia: NaNs detectados en X_test DESPUÉS del preprocesamiento.\")\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    print(\"--- Exiting preprocess_and_split ---\") # Debug print\n",
    "    return X_train_processed, X_test_processed, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Define the `train_predict_evaluate` function (MODIFICADA)\n",
    "def train_predict_evaluate(model_name, search_method, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo especificado usando un método de búsqueda de hiperparámetros,\n",
    "    realiza predicciones y evalúa el rendimiento.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Entrenando {model_name} usando {search_method} ---\")\n",
    "\n",
    "    # --- Definir Modelos y Espacios de Búsqueda ---\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'KNN': KNeighborsRegressor(n_jobs=-1)\n",
    "        # Añadir otros modelos aquí si se desea\n",
    "    }\n",
    "\n",
    "    # Rejillas para GridSearchCV\n",
    "    param_grids = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None], # None significa sin límite\n",
    "            'min_samples_leaf': [3, 5, 10],\n",
    "            'max_features': ['sqrt', 0.5] # 'sqrt' o una fracción\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7, 10],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Distribuciones para RandomizedSearchCV\n",
    "    param_dists = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators': randint(100, 301), # Enteros entre 100 y 300\n",
    "            'max_depth': [10, 15, 20, 25, 30, None], # Lista de opciones\n",
    "            'min_samples_leaf': randint(1, 11), # Enteros entre 1 y 10\n",
    "            'max_features': ['sqrt', 'log2', 0.5, 0.7] # Lista de opciones\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': randint(1, 15), # Enteros entre 1 y 14\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- Seleccionar Modelo y Configurar Búsqueda ---\n",
    "    if model_name not in models:\n",
    "        print(f\"Error: Modelo '{model_name}' no reconocido.\")\n",
    "        return\n",
    "\n",
    "    base_model = models[model_name]\n",
    "    search_cv = None\n",
    "\n",
    "    if search_method == 'GridSearch':\n",
    "        if model_name in param_grids:\n",
    "            search_cv = GridSearchCV(\n",
    "                estimator=base_model,\n",
    "                param_grid=param_grids[model_name],\n",
    "                cv=3, # Número de folds para validación cruzada\n",
    "                scoring='neg_mean_absolute_error', # MAE negativo (GridSearchCV maximiza)\n",
    "                verbose=1,\n",
    "                n_jobs=-1 # Usar todos los cores disponibles\n",
    "            )\n",
    "            print(f\"Configurando GridSearchCV para {model_name}...\")\n",
    "        else:\n",
    "            print(f\"Advertencia: No hay param_grid definido para {model_name}. Usando modelo base.\")\n",
    "            search_cv = base_model # Usar modelo con parámetros por defecto\n",
    "\n",
    "    elif search_method == 'RandomSearch':\n",
    "        if model_name in param_dists:\n",
    "            search_cv = RandomizedSearchCV(\n",
    "                estimator=base_model,\n",
    "                param_distributions=param_dists[model_name],\n",
    "                n_iter=20, # Número de combinaciones a probar (ajustar según tiempo/recursos)\n",
    "                cv=3,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                verbose=1,\n",
    "                n_jobs=-1,\n",
    "                random_state=42 # Para reproducibilidad\n",
    "            )\n",
    "            print(f\"Configurando RandomizedSearchCV para {model_name}...\")\n",
    "        else:\n",
    "            print(f\"Advertencia: No hay param_dist definido para {model_name}. Usando modelo base.\")\n",
    "            search_cv = base_model\n",
    "\n",
    "    elif search_method == 'None':\n",
    "         print(f\"Usando modelo {model_name} con parámetros por defecto.\")\n",
    "         search_cv = base_model # Entrenar directamente sin búsqueda\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Método de búsqueda '{search_method}' no reconocido.\")\n",
    "        return\n",
    "\n",
    "    # --- Entrenar ---\n",
    "    print(\"Entrenando...\")\n",
    "    search_cv.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener el mejor modelo (si se usó búsqueda)\n",
    "    if hasattr(search_cv, 'best_estimator_'):\n",
    "        model = search_cv.best_estimator_\n",
    "        print(\"Mejores parámetros encontrados:\")\n",
    "        print(search_cv.best_params_)\n",
    "    else:\n",
    "        model = search_cv # Caso 'None' o si no había grid/dist\n",
    "\n",
    "    # --- Predecir y Evaluar ---\n",
    "    print(\"Realizando predicciones...\")\n",
    "    predictions_log = model.predict(X_test)\n",
    "\n",
    "    # Revertir la transformación logarítmica y asegurar no negativos/enteros\n",
    "    # Es crucial revertir tanto las predicciones como y_test para comparar en la escala original\n",
    "    predictions = np.maximum(0, np.expm1(predictions_log).round().astype(int))\n",
    "    y_test_original = np.expm1(y_test) # Revertir y_test también\n",
    "\n",
    "    # Calcular y mostrar el MAE en la escala original\n",
    "    mae = mean_absolute_error(y_test_original, predictions)\n",
    "    print(f'\\nMean Absolute Error (MAE) - {model_name} ({search_method}): {mae:.4f}')\n",
    "\n",
    "    # Mostrar estadísticas del conjunto de prueba en la escala original\n",
    "    stats = pd.Series(y_test_original).describe()\n",
    "    print(f\"\\nEstadísticas de 'total_cases' en el conjunto de prueba (escala original):\")\n",
    "    # Imprimir estadísticas de forma más legible\n",
    "    print(f\"- Media:      {stats['mean']:.4f}\")\n",
    "    print(f\"- Mediana:    {stats['50%']:.4f}\")\n",
    "    print(f\"- Desv. Est.: {stats['std']:.4f}\")\n",
    "    print(f\"- Mínimo:     {stats['min']:.4f}\")\n",
    "    print(f\"- Máximo:     {stats['max']:.4f}\")\n",
    "\n",
    "    if stats['mean'] > 0:\n",
    "        mae_perc_mean = (mae / stats['mean']) * 100\n",
    "        print(f\"\\n- MAE como % de la Media: {mae_perc_mean:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n- No se puede calcular MAE como % de la Media (la media es 0).\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Load and Merge Data\n",
    "Load the datasets using the `load_and_merge_data` function and display the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES_FILE = 'dengue_features_train.csv'\n",
    "LABELS_FILE = 'dengue_labels_train.csv'\n",
    "\n",
    "# 1. Cargar y unir los datos\n",
    "# En la celda donde llamas a preprocess_and_split (ej. Celda 24)\n",
    "\n",
    "merged_df = load_and_merge_data(FEATURES_FILE, LABELS_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Create Preprocessor\n",
    "Create the preprocessor using the `create_preprocessor` function and display its configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow - Train, Predict, and Evaluate\n",
    "Train the model, make predictions, and evaluate its performance using the `train_predict_evaluate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor created.\n",
      "--- Inside preprocess_and_split ---\n",
      "Value of target_col at entry: total_cases\n",
      "Type of target_col at entry: <class 'str'>\n",
      "Attempting to access df column using target_col: 'total_cases'\n",
      "Fitting preprocessor...\n",
      "Transforming train data...\n",
      "Transforming test data...\n",
      "--- Exiting preprocess_and_split ---\n",
      "X_train shape: (1164, 27)\n",
      "\n",
      "--- Entrenando RandomForest usando GridSearch ---\n",
      "Configurando GridSearchCV para RandomForest...\n",
      "Entrenando...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javier\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2892: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados:\n",
      "{'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "Realizando predicciones...\n",
      "\n",
      "Mean Absolute Error (MAE) - RandomForest (GridSearch): 11.6199\n",
      "\n",
      "Estadísticas de 'total_cases' en el conjunto de prueba (escala original):\n",
      "- Media:      25.4829\n",
      "- Mediana:    13.0000\n",
      "- Desv. Est.: 41.7328\n",
      "- Mínimo:     0.0000\n",
      "- Máximo:     410.0000\n",
      "\n",
      "- MAE como % de la Media: 45.60%\n",
      "----------------------------------------\n",
      "\n",
      "--- Entrenando RandomForest usando RandomSearch ---\n",
      "Configurando RandomizedSearchCV para RandomForest...\n",
      "Entrenando...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Mejores parámetros encontrados:\n",
      "{'max_depth': 10, 'max_features': 0.7, 'min_samples_leaf': 2, 'n_estimators': 108}\n",
      "Realizando predicciones...\n",
      "\n",
      "Mean Absolute Error (MAE) - RandomForest (RandomSearch): 11.1747\n",
      "\n",
      "Estadísticas de 'total_cases' en el conjunto de prueba (escala original):\n",
      "- Media:      25.4829\n",
      "- Mediana:    13.0000\n",
      "- Desv. Est.: 41.7328\n",
      "- Mínimo:     0.0000\n",
      "- Máximo:     410.0000\n",
      "\n",
      "- MAE como % de la Media: 43.85%\n",
      "----------------------------------------\n",
      "\n",
      "--- Entrenando KNN usando GridSearch ---\n",
      "Configurando GridSearchCV para KNN...\n",
      "Entrenando...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Mejores parámetros encontrados:\n",
      "{'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "Realizando predicciones...\n",
      "\n",
      "Mean Absolute Error (MAE) - KNN (GridSearch): 15.3082\n",
      "\n",
      "Estadísticas de 'total_cases' en el conjunto de prueba (escala original):\n",
      "- Media:      25.4829\n",
      "- Mediana:    13.0000\n",
      "- Desv. Est.: 41.7328\n",
      "- Mínimo:     0.0000\n",
      "- Máximo:     410.0000\n",
      "\n",
      "- MAE como % de la Media: 60.07%\n",
      "----------------------------------------\n",
      "\n",
      "--- Entrenando KNN usando RandomSearch ---\n",
      "Configurando RandomizedSearchCV para KNN...\n",
      "Entrenando...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Mejores parámetros encontrados:\n",
      "{'metric': 'manhattan', 'n_neighbors': 8, 'weights': 'distance'}\n",
      "Realizando predicciones...\n",
      "\n",
      "Mean Absolute Error (MAE) - KNN (RandomSearch): 15.8116\n",
      "\n",
      "Estadísticas de 'total_cases' en el conjunto de prueba (escala original):\n",
      "- Media:      25.4829\n",
      "- Mediana:    13.0000\n",
      "- Desv. Est.: 41.7328\n",
      "- Mínimo:     0.0000\n",
      "- Máximo:     410.0000\n",
      "\n",
      "- MAE como % de la Media: 62.05%\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if merged_df is not None:\n",
    "    # 1. Crear el preprocesador ANTES de llamar a preprocess_and_split\n",
    "    #    Asegúrate de que las columnas usadas aquí coincidan con las que tendrá X dentro de la función\n",
    "    preprocessor = create_preprocessor(merged_df.drop(columns=['total_cases', 'week_start_date']))\n",
    "    print(\"Preprocessor created.\") # Mensaje de confirmación\n",
    "\n",
    "    # 2. Preprocesar y dividir los datos, pasando el preprocesador\n",
    "    try:\n",
    "        # Ahora pasamos 'preprocessor' como segundo argumento\n",
    "        X_train, X_test, y_train, y_test = preprocess_and_split(merged_df, preprocessor)\n",
    "\n",
    "        if X_train is not None: # Verificar que el preprocesamiento fue exitoso\n",
    "            print(f\"X_train shape: {X_train.shape}\") # Imprimir shapes para verificar\n",
    "\n",
    "            # 3. Entrenar, predecir y evaluar diferentes modelos y métodos de búsqueda\n",
    "            \n",
    "            train_predict_evaluate('RandomForest', 'GridSearch', X_train, X_test, y_train, y_test)\n",
    "            train_predict_evaluate('RandomForest', 'RandomSearch', X_train, X_test, y_train, y_test)\n",
    "            train_predict_evaluate('KNN', 'GridSearch', X_train, X_test, y_train, y_test)\n",
    "            train_predict_evaluate('KNN', 'RandomSearch', X_train, X_test, y_train, y_test)\n",
    "\n",
    "        else:\n",
    "            print(\"Error durante el preprocesamiento y división de datos.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during preprocess_and_split call or subsequent steps: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"Proceso detenido debido a error en la carga de archivos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
