{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzGSuGD4Agbq"
      },
      "source": [
        "#  Taller: Limpieza y Validación de Datos Complejos en Big Data con PySpark\n",
        "### Google Colab Notebook\n",
        "---\n",
        "**Objetivo:** Ejecutar un flujo completo de limpieza y validación de datos usando PySpark, detectando errores comunes y avanzados en datasets simulados, generando un archivo limpio en formato Parquet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUcIoyLxAgbv"
      },
      "source": [
        "##  1. Instalación y Configuración del Entorno"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7Bn7pE4ScR",
        "outputId": "031cb94e-8716-4f1f-db2f-2cf9a2caec46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-30 18:06:47--  https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400724056 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.5-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.5-bin-had 100%[===================>] 382.16M  18.8MB/s    in 22s     \n",
            "\n",
            "2025-04-30 18:07:09 (17.5 MB/s) - ‘spark-3.5.5-bin-hadoop3.tgz’ saved [400724056/400724056]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh spark-3.5.5-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
        "!ls /content/spark-3.5.5-bin-hadoop3/python/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk1O99Rf4VRh",
        "outputId": "6dc92b36-09fe-4496-d168-3d777bd6d546"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 383M Feb 23 20:47 spark-3.5.5-bin-hadoop3.tgz\n",
            "py4j-0.10.9.7-src.zip  PY4J_LICENSE.txt  pyspark.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark findspark pandas numpy"
      ],
      "metadata": {
        "id": "eLMxRT7b46CR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/spark-3.5.5-bin-hadoop3/python/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYKZeOny2vZS",
        "outputId": "c11a0e7c-765a-45e1-cbfb-7bf04ad30850"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "py4j-0.10.9.7-src.zip  PY4J_LICENSE.txt  pyspark.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XQoReWN3Agbx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaVGLW98Agbx"
      },
      "source": [
        "##  2. Generación Automática del Dataset Complejo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "isa-ZUSJAgbx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "n_registros = 500\n",
        "order_ids = np.arange(1000, 1000 + n_registros)\n",
        "customer_ids = np.random.choice(['CUST-' + str(np.random.randint(1000, 9999)) for _ in range(100)], n_registros)\n",
        "product_ids = np.random.choice(['PROD-' + str(np.random.randint(10, 99)) + chr(np.random.randint(65, 90)) for _ in range(50)], n_registros)\n",
        "quantities = np.random.choice([1, 2, 3, np.nan], n_registros, p=[0.3, 0.3, 0.3, 0.1])\n",
        "unit_prices = np.round(np.random.uniform(10, 300, n_registros), 2)\n",
        "outlier_indices = np.random.choice(n_registros, size=10, replace=False)\n",
        "unit_prices[outlier_indices] = np.random.choice([-50, 1200], size=10)\n",
        "dates_iso = pd.date_range('2025-03-01', periods=n_registros//2).strftime('%Y-%m-%d').tolist()\n",
        "dates_euro = pd.date_range('2025-03-01', periods=n_registros//2).strftime('%d/%m/%Y').tolist()\n",
        "order_dates = np.random.choice(dates_iso + dates_euro, n_registros)\n",
        "locations = ['Madrid-Centro', 'Barcelona @Sants', 'Valencia  Puerto', 'sevilla-triana', 'BILBAO-Casco']\n",
        "store_locations = np.random.choice(locations + [np.nan], n_registros, p=[0.2,0.2,0.2,0.2,0.1,0.1])\n",
        "data = pd.DataFrame({\n",
        "    'order_id': order_ids,\n",
        "    'customer_id': customer_ids,\n",
        "    'product_id': product_ids,\n",
        "    'quantity': quantities,\n",
        "    'unit_price': unit_prices,\n",
        "    'order_date': order_dates,\n",
        "    'store_location': store_locations\n",
        "})\n",
        "duplicates = data.sample(frac=0.1, random_state=42)\n",
        "data = pd.concat([data, duplicates], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "data.to_csv('retail_sales_complex.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FyVVMx3AAgby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939b7b29-cc6a-40c3-8a52-cf9abc06dd8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "|order_id|customer_id|product_id|quantity|unit_price|order_date|  store_location|\n",
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "|    1195|  CUST-7863|  PROD-48A|     3.0|     17.44|11/09/2025|Barcelona @Sants|\n",
            "|    1079|  CUST-6390|  PROD-59B|     3.0|    234.31|22/05/2025|   Madrid-Centro|\n",
            "|    1480|  CUST-1769|  PROD-63V|     2.0|    188.04|25/03/2025|Valencia  Puerto|\n",
            "|    1109|  CUST-6618|  PROD-14E|     1.0|     -50.0|2025-06-10|  sevilla-triana|\n",
            "|    1280|  CUST-7873|  PROD-48A|     1.0|    127.25|22/10/2025|             nan|\n",
            "|    1440|  CUST-8916|  PROD-15L|     1.0|     85.35|2025-05-12|             nan|\n",
            "|    1084|  CUST-1860|  PROD-51I|     3.0|     31.31|2025-07-12|Valencia  Puerto|\n",
            "|    1368|  CUST-4099|  PROD-79S|     2.0|     98.36|2025-10-25|   Madrid-Centro|\n",
            "|    1132|  CUST-9792|  PROD-17P|     2.0|     205.3|28/08/2025|             nan|\n",
            "|    1364|  CUST-6618|  PROD-42F|    NULL|    264.61|17/08/2025|  sevilla-triana|\n",
            "|    1184|  CUST-8513|  PROD-34X|     3.0|    118.24|05/10/2025|Barcelona @Sants|\n",
            "|    1010|  CUST-1189|  PROD-15L|     2.0|    222.16|14/08/2025|   Madrid-Centro|\n",
            "|    1073|  CUST-8629|  PROD-54M|     1.0|     57.42|26/09/2025|  sevilla-triana|\n",
            "|    1220|  CUST-5555|  PROD-63V|     3.0|     99.11|03/08/2025|   Madrid-Centro|\n",
            "|    1278|  CUST-5297|  PROD-48A|     2.0|     49.57|2025-07-19|    BILBAO-Casco|\n",
            "|    1082|  CUST-1161|  PROD-32P|     1.0|     141.6|2025-06-23|Barcelona @Sants|\n",
            "|    1382|  CUST-2267|  PROD-48C|     3.0|    231.79|04/04/2025|Valencia  Puerto|\n",
            "|    1006|  CUST-8270|  PROD-63G|    NULL|    247.02|2025-08-03|Barcelona @Sants|\n",
            "|    1218|  CUST-1995|  PROD-84R|     2.0|     84.47|27/04/2025|             nan|\n",
            "|    1448|  CUST-9322|  PROD-63V|     2.0|    242.45|07/04/2025|   Madrid-Centro|\n",
            "|    1396|  CUST-9529|  PROD-72Q|     2.0|    156.78|2025-04-08|  sevilla-triana|\n",
            "|    1167|  CUST-9684|  PROD-71Y|     1.0|    104.84|22/08/2025|    BILBAO-Casco|\n",
            "|    1468|  CUST-3731|  PROD-70L|     1.0|    101.47|09/04/2025|    BILBAO-Casco|\n",
            "|    1172|  CUST-3731|  PROD-74V|     2.0|    136.32|2025-03-16|Valencia  Puerto|\n",
            "|    1081|  CUST-3568|  PROD-24A|     1.0|    291.25|2025-09-26|Valencia  Puerto|\n",
            "|    1077|  CUST-9792|  PROD-17P|     3.0|    200.45|13/06/2025|             nan|\n",
            "|    1185|  CUST-9154|  PROD-96Y|     1.0|     75.91|05/03/2025|Barcelona @Sants|\n",
            "|    1289|  CUST-7736|  PROD-63G|     1.0|     89.51|11/06/2025|Barcelona @Sants|\n",
            "|    1377|  CUST-4561|  PROD-13D|     1.0|     48.43|2025-07-21|Valencia  Puerto|\n",
            "|    1124|  CUST-7863|  PROD-54M|     3.0|    159.52|2025-09-22|             nan|\n",
            "|    1250|  CUST-4104|  PROD-74V|     2.0|     129.2|12/03/2025|Valencia  Puerto|\n",
            "|    1481|  CUST-1995|  PROD-94K|     1.0|     29.33|2025-08-04|   Madrid-Centro|\n",
            "|    1328|  CUST-6051|  PROD-66R|     1.0|     78.05|2025-06-16|Barcelona @Sants|\n",
            "|    1072|  CUST-8555|  PROD-54M|     1.0|    181.96|20/10/2025|   Madrid-Centro|\n",
            "|    1148|  CUST-5297|  PROD-66R|     2.0|    279.98|2025-05-28|   Madrid-Centro|\n",
            "|    1076|  CUST-1769|  PROD-26G|     1.0|    1200.0|21/07/2025|             nan|\n",
            "|    1002|  CUST-3731|  PROD-47S|     1.0|     246.3|05/04/2025|Valencia  Puerto|\n",
            "|    1165|  CUST-2267|  PROD-54M|     3.0|    264.77|2025-06-18|   Madrid-Centro|\n",
            "|    1416|  CUST-7863|  PROD-63V|    NULL|      39.3|10/05/2025|   Madrid-Centro|\n",
            "|    1428|  CUST-1860|  PROD-72Q|     2.0|    267.12|29/03/2025|             nan|\n",
            "|    1199|  CUST-8208|  PROD-48C|     3.0|    178.25|01/10/2025|  sevilla-triana|\n",
            "|    1209|  CUST-1189|  PROD-48C|     1.0|    197.77|21/05/2025|    BILBAO-Casco|\n",
            "|    1354|  CUST-8869|  PROD-12M|     1.0|    159.55|2025-08-24|             nan|\n",
            "|    1077|  CUST-9792|  PROD-17P|     3.0|    200.45|13/06/2025|             nan|\n",
            "|    1153|  CUST-9529|  PROD-47S|     1.0|    124.22|25/10/2025|    BILBAO-Casco|\n",
            "|    1386|  CUST-9529|  PROD-28T|     3.0|     52.45|26/04/2025|Barcelona @Sants|\n",
            "|    1491|  CUST-6051|  PROD-28T|     3.0|     30.66|2025-07-04|Barcelona @Sants|\n",
            "|    1068|  CUST-1161|  PROD-23E|     1.0|    262.52|03/09/2025|Valencia  Puerto|\n",
            "|    1280|  CUST-7873|  PROD-48A|     1.0|    127.25|22/10/2025|             nan|\n",
            "|    1406|  CUST-8849|  PROD-94K|     3.0|    143.34|15/07/2025|  sevilla-triana|\n",
            "+--------+-----------+----------+--------+----------+----------+----------------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataQualityComplex\").getOrCreate()\n",
        "df = spark.read.csv(\"retail_sales_complex.csv\", header=True, inferSchema=True)\n",
        "df.show(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RqfOVmrAgby"
      },
      "source": [
        "##  3. Diagnóstico de Calidad de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttxqc6NPAgbz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "print(f\"Duplicados detectados: {df.count() - df.dropDuplicates().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos copia del dataset original antes de limpiar\n",
        "df_original = df\n",
        "total_registros = df_original.count()"
      ],
      "metadata": {
        "id": "Sk6mZQuV_siI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos copia del dataset original antes de limpiar\n",
        "df_original = df\n",
        "total_registros = df_original.count()"
      ],
      "metadata": {
        "id": "zbE-AYB5S2gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4nd6zArAgbz"
      },
      "source": [
        "##  4. Limpieza Avanzada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMv3k7u4Agbz"
      },
      "outputs": [],
      "source": [
        "# Rellenar valores nulos\n",
        "mean_quantity = df.agg({'quantity': 'mean'}).first()[0]\n",
        "df = df.fillna({'quantity': round(mean_quantity, 0), 'store_location': 'Desconocido'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gqlDhWnAgbz"
      },
      "outputs": [],
      "source": [
        "# Eliminar duplicados\n",
        "df = df.dropDuplicates(['order_id', 'customer_id', 'order_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYllaVnlAgb0"
      },
      "outputs": [],
      "source": [
        "# Normalización de fechas\n",
        "from pyspark.sql.functions import to_date\n",
        "df = df.withColumn('order_date',\n",
        "    when(col('order_date').rlike('\\d{2}/\\d{2}/\\d{4}'),\n",
        "         to_date(col('order_date'), 'dd/MM/yyyy'))\n",
        "    .otherwise(to_date(col('order_date'), 'yyyy-MM-dd'))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRKLe0ItAgb0"
      },
      "outputs": [],
      "source": [
        "# Limpieza de store_location\n",
        "from pyspark.sql.functions import regexp_replace, lower, trim\n",
        "df = df.withColumn('store_location', regexp_replace('store_location', '@', '-'))\n",
        "df = df.withColumn('store_location', regexp_replace('store_location', '  ', '-'))\n",
        "df = df.withColumn('store_location', lower(trim(col('store_location'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZOeeVhLAgb0"
      },
      "outputs": [],
      "source": [
        "# Tratamiento de outliers\n",
        "Q1 = df.approxQuantile(\"unit_price\", [0.25], 0.05)[0]\n",
        "Q3 = df.approxQuantile(\"unit_price\", [0.75], 0.05)[0]\n",
        "IQR = Q3 - Q1\n",
        "df = df.filter((col(\"unit_price\") >= (Q1 - 1.5 * IQR)) & (col(\"unit_price\") <= (Q3 + 1.5 * IQR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VabRniJSAgb0"
      },
      "source": [
        "##  5. Análisis de Métricas de Calidad Pre y Post Limpieza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np1hSKL1Agb1"
      },
      "source": [
        "###  Porcentaje de Nulos, Duplicados y Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEjpqprAAgb1"
      },
      "outputs": [],
      "source": [
        "# 1. Porcentaje de valores nulos\n",
        "nulos = df_original.select([count(when(col(c).isNull(), c)).alias(c) for c in df_original.columns])\n",
        "nulos_percent = nulos.select([(col(c)/total_registros*100).alias(c + '_pct') for c in df_original.columns])\n",
        "nulos_percent.show()\n",
        "\n",
        "# 2. Duplicados\n",
        "duplicados = total_registros - df_original.dropDuplicates().count()\n",
        "print(f'Duplicados detectados: {duplicados} ({round(duplicados/total_registros*100, 2)}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHdqHFcUAgb1"
      },
      "source": [
        "###  Estadísticas Descriptivas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4OkPwiqAgb1"
      },
      "outputs": [],
      "source": [
        "print('Antes de la limpieza:')\n",
        "df_original.describe(['quantity', 'unit_price']).show()\n",
        "\n",
        "print('Después de la limpieza:')\n",
        "df.describe(['quantity', 'unit_price']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o5evV-XAgb1"
      },
      "source": [
        "###  Conteo de Valores Únicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwjBXMFBAgb2"
      },
      "outputs": [],
      "source": [
        "print('Ubicaciones únicas tras limpieza:')\n",
        "df.select('store_location').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3NMMVVxAgb2"
      },
      "outputs": [],
      "source": [
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "df.write.mode('overwrite').parquet('retail_sales_cleaned.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comprobaciones Finales y exportación de los datos"
      ],
      "metadata": {
        "id": "n_2TvRTQAUyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobación final de nulos\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# Comprobación de tamaño final del dataset\n",
        "print(f\"Número final de registros: {df.count()}\")\n",
        "\n",
        "# Exportación a formato Parquet\n",
        "df.write.mode('overwrite').parquet('retail_sales_cleaned.parquet')\n"
      ],
      "metadata": {
        "id": "J085TU2-ANeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J01GYDKdSIJq"
      },
      "source": [
        "##  7. Comparativa: Ventajas de Convertir CSV a Parquet\n",
        "\n",
        "En esta sección analizaremos las diferencias prácticas entre trabajar con un archivo CSV y un archivo Parquet en términos de **tamaño**, **velocidad de carga** y **eficiencia en la lectura de datos**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0akOhKC9SIJr"
      },
      "source": [
        "###  Comparar Tamaño de Archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxQhAp-MSIJr"
      },
      "outputs": [],
      "source": [
        "!ls -lh retail_sales_complex.csv\n",
        "!du -h retail_sales_cleaned.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcYUtt8MSIJr"
      },
      "source": [
        "###  Comparar Tiempo de Carga en PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCwV8IoMSIJs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_csv = time.time()\n",
        "df_csv = spark.read.csv(\"retail_sales_complex.csv\", header=True, inferSchema=True)\n",
        "end_csv = time.time()\n",
        "print(f\"Tiempo de carga CSV: {end_csv - start_csv:.4f} segundos\")\n",
        "\n",
        "start_parquet = time.time()\n",
        "df_parquet = spark.read.parquet(\"retail_sales_cleaned.parquet\")\n",
        "end_parquet = time.time()\n",
        "print(f\"Tiempo de carga Parquet: {end_parquet - start_parquet:.4f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4RkrmDCSIJs"
      },
      "source": [
        "###  Comparar Lectura de Columnas Específicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aanbAo82SIJs"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "df_csv.select(\"order_id\", \"quantity\").show(5)\n",
        "print(\"CSV → lectura columnas:\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "df_parquet.select(\"order_id\", \"quantity\").show(5)\n",
        "print(\"Parquet → lectura columnas:\", time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBGgjHdQSIJu"
      },
      "source": [
        "###  Conclusión\n",
        "\n",
        "Reflexiona sobre las diferencias observadas en términos de espacio, tiempos de carga y eficiencia.\n",
        "\n",
        "- ¿Qué ventajas ofrece el formato Parquet frente al CSV?\n",
        "- ¿En qué tipo de proyectos Big Data sería más adecuado usar Parquet?\n",
        "\n",
        "Incluye esta reflexión en tu informe final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw0_GAkwAgb2"
      },
      "source": [
        "##  8. Informe del Alumno\n",
        "- **Errores detectados:**\n",
        "- **Acciones aplicadas:**\n",
        "- **Impacto en el negocio:**\n",
        "- **Capturas de evidencia:**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}